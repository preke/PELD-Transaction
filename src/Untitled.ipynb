{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b80c32685948739b418b31615047c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/730M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# Download and setup the model and tokenizer\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model =  BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69d17488a164489a032cb76fd24508c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66e5e2d953247b9a1f034ebada38068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset daily_dialog/default (download: 4.27 MiB, generated: 8.23 MiB, post-processed: Unknown size, total: 12.50 MiB) to /home/zhiyuan/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4089bde53bfe4c52a4cfeca8cabadd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset daily_dialog downloaded and prepared to /home/zhiyuan/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyuan/ENTER/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2675,\n",
       " 588,\n",
       " 2755,\n",
       " 304,\n",
       " 3379,\n",
       " 1016,\n",
       " 459,\n",
       " 3994,\n",
       " 2453,\n",
       " 228,\n",
       " 2,\n",
       " 1552,\n",
       " 466,\n",
       " 2453,\n",
       " 228,\n",
       " 2,\n",
       " 683,\n",
       " 296,\n",
       " 5623,\n",
       " 553,\n",
       " 513,\n",
       " 2453,\n",
       " 7773,\n",
       " 2755,\n",
       " 485,\n",
       " 295,\n",
       " 5158,\n",
       " 2755,\n",
       " 397,\n",
       " 838,\n",
       " 587,\n",
       " 1446,\n",
       " 459,\n",
       " 677,\n",
       " 2013,\n",
       " 317,\n",
       " 5623,\n",
       " 228,\n",
       " 2,\n",
       " 1395,\n",
       " 2755,\n",
       " 332,\n",
       " 1525,\n",
       " 2755,\n",
       " 528,\n",
       " 2234,\n",
       " 2017,\n",
       " 228,\n",
       " 2,\n",
       " 281,\n",
       " 682,\n",
       " 360,\n",
       " 4019,\n",
       " 421,\n",
       " 304,\n",
       " 2434,\n",
       " 287,\n",
       " 361,\n",
       " 265,\n",
       " 888,\n",
       " 3312,\n",
       " 2017,\n",
       " 228,\n",
       " 2,\n",
       " 880,\n",
       " 2755,\n",
       " 281,\n",
       " 632,\n",
       " 1609,\n",
       " 2755,\n",
       " 551,\n",
       " 2017,\n",
       " 228,\n",
       " 2,\n",
       " 5611,\n",
       " 324,\n",
       " 588,\n",
       " 5623,\n",
       " 281,\n",
       " 600,\n",
       " 660,\n",
       " 361,\n",
       " 980,\n",
       " 298,\n",
       " 7668,\n",
       " 5623,\n",
       " 3137,\n",
       " 459,\n",
       " 5623,\n",
       " 228,\n",
       " 2,\n",
       " 946,\n",
       " 304,\n",
       " 551,\n",
       " 360,\n",
       " 430,\n",
       " 306,\n",
       " 841,\n",
       " 3552,\n",
       " 2453,\n",
       " 2354,\n",
       " 361,\n",
       " 304,\n",
       " 407,\n",
       " 494,\n",
       " 482,\n",
       " 2453,\n",
       " 228,\n",
       " 2,\n",
       " 281,\n",
       " 660,\n",
       " 395,\n",
       " 3808,\n",
       " 667,\n",
       " 5623,\n",
       " 1021,\n",
       " 958,\n",
       " 383,\n",
       " 466,\n",
       " 304,\n",
       " 538,\n",
       " 298,\n",
       " 281,\n",
       " 228,\n",
       " 515,\n",
       " 228,\n",
       " 289,\n",
       " 600,\n",
       " 945,\n",
       " 304,\n",
       " 487,\n",
       " 228,\n",
       " 4075,\n",
       " 335,\n",
       " 1176,\n",
       " 2017,\n",
       " 228,\n",
       " 2,\n",
       " 3684,\n",
       " 584,\n",
       " 5623,\n",
       " 2183,\n",
       " 228,\n",
       " 515,\n",
       " 268,\n",
       " 595,\n",
       " 2755,\n",
       " 281,\n",
       " 538,\n",
       " 2017,\n",
       " 228,\n",
       " 2,\n",
       " 1167,\n",
       " 2453,\n",
       " 228,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "test = load_dataset('daily_dialog', split='test')\n",
    "test\n",
    "\n",
    "utterance = \"I want to order a Pizza\"\n",
    "# pt: return pytorch tensors\n",
    "# inputs = tokenizer('</s>'.join(test['dialog'][0][:-1]), return_tensors=\"pt\", max)\n",
    "inputs = tokenizer.encode('</s>'.join(test['dialog'][0][:-1]), add_special_tokens=True, max_length=256, pad_to_max_length=True)\n",
    "inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
