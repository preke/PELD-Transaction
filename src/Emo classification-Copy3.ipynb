{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyuan/ENTER/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': array([-0.59  ,  0.3725, -0.1   ]), 'positive': array([0.605, 0.59 , 0.165]), 'neutral': array([0, 0, 0])}\n",
      "(6510, 3)\n",
      "(6185, 3)\n",
      "(6185, 3)\n",
      "6185 6185\n",
      "6185\n"
     ]
    }
   ],
   "source": [
    "# coding = utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import datetime\n",
    "import traceback\n",
    "import model\n",
    "import random\n",
    "\n",
    "DATA_PATH = '../Dyadic_PELD.tsv'\n",
    "\n",
    "# identify and specify the GPU as the device, later in training loop we will load data into device\n",
    "SEED = 19\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "MAX_LEN = 256\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, sep='\\t').fillna('Nan')\n",
    "    \n",
    "Utterance_1   = df['Utterance_1'].values \n",
    "Utterance_2   = df['Utterance_2'].values\n",
    "Utterance_3   = df['Utterance_3'].values\n",
    "\n",
    "label_1 = df['Emotion_1']\n",
    "label_2 = df['Emotion_2']\n",
    "label_3 = df['Emotion_3']\n",
    "\n",
    "\n",
    "VAD_Lexicons = pd.read_csv('NRC-VAD-Lexicon.txt', sep='\\t')\n",
    "VAD_dict = {}\n",
    "for r in VAD_Lexicons.iterrows():\n",
    "    VAD_dict[r[1]['Word']] = [r[1]['Valence'], r[1]['Arousal'], r[1]['Dominance']]\n",
    "\n",
    "from utils import get_vad, Emotion_dict, Emotion_Senti\n",
    "\n",
    "'''\n",
    "Emotion_dict = {\n",
    "    'anger': [-0.51, 0.59, 0.25],\n",
    "    'disgust': [-0.60, 0.35, 0.11],\n",
    "    'fear': [-0.62, 0.82, -0.43],\n",
    "    'joy': [0.81, 0.51, 0.46],\n",
    "    'neutral': [0.0, 0.0, 0.0],\n",
    "    'sadness': [-0.63, -0.27, -0.33],\n",
    "    'surprise': [0.40, 0.67, -0.13]\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "Mood_dict = {}\n",
    "Mood_dict['negative'] = np.average([\n",
    "                np.array(Emotion_dict['anger']), \n",
    "                np.array(Emotion_dict['disgust']), \n",
    "                np.array(Emotion_dict['fear']), \n",
    "                np.array(Emotion_dict['sadness'])],axis=0)\n",
    "Mood_dict['positive'] =  np.average([\n",
    "                np.array(Emotion_dict['joy']),\n",
    "                np.array(Emotion_dict['surprise'])],axis=0)\n",
    "Mood_dict['neutral'] = np.array([0,0,0])\n",
    "\n",
    "print(Mood_dict)\n",
    "\n",
    "\n",
    "# Emo recognition\n",
    "# sentences = list(Utterance_1) + list(Utterance_2) + list(Utterance_3)\n",
    "# labels = list(label_1) + list(label_2) + list(label_3)\n",
    "# df = pd.DataFrame([])\n",
    "# df['sents'] = sentences\n",
    "# df['labels'] = labels\n",
    "# print(df.shape)\n",
    "# df = df.drop_duplicates(subset=['sents'], keep='first', inplace=False)\n",
    "# print(df.shape)\n",
    "# df = df[df['labels'] != 'neutral']\n",
    "# print(df.shape)\n",
    "\n",
    "# sentences = df['sents'].values\n",
    "# labels = df['labels'].values\n",
    "# init_emo = get_vad(VAD_dict, sentences)\n",
    "\n",
    "\n",
    "# sentences = list(Utterance_1)\n",
    "# labels = list(label_1)\n",
    "\n",
    "sentences = Utterance_1 + ' [SEP] ' + Utterance_2\n",
    "labels = list(label_3)\n",
    "df = pd.DataFrame([])\n",
    "df['sents'] = sentences\n",
    "df['labels'] = labels\n",
    "df['init_emo'] = label_1\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates(subset=['sents'], keep='first', inplace=False)\n",
    "print(df.shape)\n",
    "# df = df[df['labels'] != 'neutral']\n",
    "print(df.shape)\n",
    "\n",
    "sentences = df['sents'].values\n",
    "labels = df['labels'].values\n",
    "init_emo = get_vad(VAD_dict, sentences)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "labels = labelencoder.fit_transform(labels)\n",
    "labels = list(labels)\n",
    "\n",
    "\n",
    "print(len(sentences), len(set(sentences)))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.17087999999999998, 0.10100000000000002, 0.17167999999999994],\n",
       " [0.12188235294117647, 0.06994117647058824, 0.07876470588235296],\n",
       " [0.20342857142857143, 0.11076190476190477, 0.18633333333333332],\n",
       " [0.20180952380952383, 0.12138095238095238, 0.16723809523809524],\n",
       " [0.2723333333333333, 0.17422222222222222, 0.23666666666666666],\n",
       " [0.20172222222222222, 0.11866666666666667, 0.19066666666666665],\n",
       " [0.13118749999999998, 0.0716875, 0.0916875],\n",
       " [0.05884615384615385, 0.02776923076923077, 0.05415384615384616],\n",
       " [0.0478125, 0.0225625, 0.044000000000000004],\n",
       " [0.0875, 0.1112, 0.0746],\n",
       " [0.07954545454545454, 0.10109090909090908, 0.06781818181818182],\n",
       " [0.059, 0.07175, 0.062083333333333345],\n",
       " [0.108, 0.10327272727272728, 0.10590909090909091],\n",
       " [0.20835714285714288, 0.1462857142857143, 0.14092857142857143],\n",
       " [0.17061904761904761, 0.12952380952380954, 0.1099047619047619],\n",
       " [0.09549999999999999, 0.07891666666666668, 0.06291666666666666],\n",
       " [0.1665, 0.156, 0.145875],\n",
       " [0.21516666666666664, 0.18275, 0.18100000000000002],\n",
       " [0.1388888888888889, 0.10500000000000001, 0.11166666666666665],\n",
       " [0.072875, 0.045875000000000006, 0.06825],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1063529411764706, 0.044941176470588234, 0.08729411764705883],\n",
       " [0.13907692307692307, 0.05876923076923077, 0.11415384615384615],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.07285714285714286, 0.063, 0.06342857142857143],\n",
       " [0.13439024390243903, 0.10541463414634146, 0.10995121951219512],\n",
       " [0.14061904761904762, 0.10907142857142856, 0.11754761904761905],\n",
       " [0.10066666666666665, 0.07777777777777778, 0.097],\n",
       " [0.07089285714285716, 0.09385714285714286, 0.0605],\n",
       " [0.09821428571428573, 0.10675, 0.08564285714285715],\n",
       " [0.19125, 0.09025, 0.17600000000000002],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.07686666666666667, 0.06013333333333334, 0.07166666666666668],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.11049999999999999, 0.04633333333333334, 0.06],\n",
       " [0.22749999999999998, 0.1298, 0.16360000000000002],\n",
       " [0.1816923076923077, 0.10507692307692308, 0.1366153846153846],\n",
       " [0.14194117647058824, 0.06723529411764706, 0.09017647058823529],\n",
       " [0.18045833333333336, 0.08754166666666667, 0.11324999999999998],\n",
       " [0.20962962962962964, 0.12866666666666668, 0.13581481481481483],\n",
       " [0.24286666666666668, 0.12946666666666667, 0.1794666666666667],\n",
       " [0.12333333333333334, 0.05866666666666667, 0.11833333333333333],\n",
       " [0.2605, 0.16381249999999997, 0.21256250000000002],\n",
       " [0.18654545454545454, 0.12172727272727273, 0.151],\n",
       " [0.14600000000000002, 0.10470833333333333, 0.12179166666666667],\n",
       " [0.1479705882352941, 0.09588235294117646, 0.10920588235294118],\n",
       " [0.1927142857142857, 0.13944897959183675, 0.16440816326530613],\n",
       " [0.22915624999999998, 0.18753124999999998, 0.22840625],\n",
       " [0.18065217391304345, 0.13895652173913042, 0.161],\n",
       " [0.2388, 0.14528000000000002, 0.19256],\n",
       " [0.23859459459459462, 0.139972972972973, 0.19245945945945947],\n",
       " [0.19053333333333336, 0.10313333333333334, 0.1538],\n",
       " [0.006, 0.08314285714285716, 0.06857142857142857],\n",
       " [0.1985, 0.14654166666666665, 0.18775],\n",
       " [0.19924999999999998, 0.13885, 0.1777],\n",
       " [0.1704516129032258, 0.12387096774193548, 0.1536774193548387],\n",
       " [0.04585, 0.04235, 0.06505000000000001],\n",
       " [0.23150000000000004, 0.1522857142857143, 0.15992857142857142],\n",
       " [0.083375, 0.041749999999999995, 0.0686875],\n",
       " [0.17308333333333334, 0.08208333333333333, 0.117],\n",
       " [0.017499999999999998, 0.04214285714285714, 0.02642857142857143],\n",
       " [0.17355555555555552, 0.12322222222222222, 0.15133333333333335],\n",
       " [0.049124999999999995, 0.0255, 0.038625],\n",
       " [0.07275000000000001, 0.034874999999999996, 0.038],\n",
       " [0.0826, 0.11266666666666666, 0.09306666666666666],\n",
       " [0.1752857142857143, 0.17528571428571427, 0.1485],\n",
       " [0.242, 0.17733333333333334, 0.2298888888888889],\n",
       " [0.23068965517241377, 0.1663103448275862, 0.16651724137931037],\n",
       " [0.19816666666666669, 0.10816666666666667, 0.09616666666666666],\n",
       " [0.18472222222222223, 0.10244444444444445, 0.16005555555555553],\n",
       " [0.18472222222222223, 0.10244444444444445, 0.16005555555555553],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.07590476190476189, 0.0929047619047619, 0.07633333333333334],\n",
       " [0.15171428571428572, 0.13942857142857143, 0.11199999999999999],\n",
       " [0.1390392156862745, 0.1075686274509804, 0.1025294117647059],\n",
       " [0.1964615384615385, 0.10838461538461537, 0.149],\n",
       " [0.16462500000000002, 0.06075, 0.09287499999999999],\n",
       " [0.19128571428571425, 0.1359047619047619, 0.17785714285714288],\n",
       " [0.107375, 0.0538125, 0.08925],\n",
       " [0.20338095238095238, 0.15852380952380954, 0.17857142857142858],\n",
       " [0.2003, 0.13720000000000002, 0.1544],\n",
       " [0.18096153846153845, 0.11753846153846154, 0.15426923076923077],\n",
       " [0.24397297297297296, 0.15513513513513513, 0.19708108108108113],\n",
       " [0.256, 0.12257142857142857, 0.17871428571428574],\n",
       " [0.1802692307692308, 0.11380769230769229, 0.12419230769230771],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1320666666666667, 0.1084, 0.12193333333333335],\n",
       " [0.18129411764705883, 0.12082352941176469, 0.13947058823529415],\n",
       " [0.14479166666666668, 0.13129166666666667, 0.12250000000000001],\n",
       " [0.18609523809523812, 0.13280952380952382, 0.15066666666666667],\n",
       " [0.16088235294117648, 0.1605294117647059, 0.16347058823529415],\n",
       " [0.10366666666666668, 0.11916666666666667, 0.09922222222222223],\n",
       " [0.2011111111111111, 0.14640740740740743, 0.16188888888888892],\n",
       " [0.22820000000000007, 0.14219999999999997, 0.15953333333333333],\n",
       " [0.1791851851851852, 0.11155555555555553, 0.12485185185185185],\n",
       " [0.17453846153846156, 0.14846153846153848, 0.1489230769230769],\n",
       " [0.15639999999999998, 0.09435, 0.12465000000000001],\n",
       " [0.005166666666666667, 0.05366666666666667, 0.034583333333333334],\n",
       " [0.1910714285714286, 0.1387857142857143, 0.15878571428571428],\n",
       " [0.18359999999999999, 0.122, 0.11320000000000001],\n",
       " [0.2755, 0.16399999999999998, 0.20769999999999994],\n",
       " [0.02316666666666667, 0.013888888888888888, 0.028277777777777777],\n",
       " [0.06861111111111112, 0.08744444444444445, 0.09127777777777778],\n",
       " [0.12754545454545455, 0.07100000000000001, 0.09436363636363637],\n",
       " [0.07444444444444445, 0.06088888888888888, 0.059555555555555556],\n",
       " [0.11979999999999999, 0.0627, 0.10500000000000001],\n",
       " [0.14583333333333334, 0.10233333333333333, 0.108],\n",
       " [0.11653846153846155, 0.07699999999999999, 0.09546153846153847],\n",
       " [0.11560000000000001, 0.1002, 0.0749],\n",
       " [0.082, 0.10446666666666668, 0.05606666666666667],\n",
       " [0.0989047619047619, 0.13504761904761906, 0.0940952380952381],\n",
       " [0.1396, 0.10344000000000002, 0.12431999999999999],\n",
       " [0.18615384615384614, 0.10342307692307692, 0.15634615384615383],\n",
       " [0.22878947368421051, 0.17421052631578948, 0.20626315789473684],\n",
       " [0.19142857142857142, 0.12985714285714284, 0.17823809523809525],\n",
       " [0.11978947368421052, 0.06057894736842105, 0.112],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1721, 0.14200000000000002, 0.18130000000000002],\n",
       " [0.17627272727272728, 0.1080909090909091, 0.14877272727272728],\n",
       " [0.1572, 0.08815, 0.10969999999999999],\n",
       " [0.108125, 0.0625, 0.07225],\n",
       " [0.18380000000000002, 0.156, 0.1533],\n",
       " [0.2091764705882353, 0.1341176470588235, 0.15611764705882353],\n",
       " [0.1348529411764706, 0.10602941176470589, 0.1350294117647059],\n",
       " [0.13893333333333333, 0.08546666666666666, 0.098],\n",
       " [0.22194117647058823, 0.19511764705882353, 0.2005294117647059],\n",
       " [0.1705, 0.20200000000000004, 0.200875],\n",
       " [0.19500000000000003, 0.10735714285714286, 0.14714285714285716],\n",
       " [0.09007407407407407, 0.06337037037037037, 0.06781481481481481],\n",
       " [0.18420689655172415, 0.14151724137931035, 0.1494137931034483],\n",
       " [0.09377777777777778, 0.1111111111111111, 0.07600000000000001],\n",
       " [0.23233333333333336, 0.1210416666666667, 0.17895833333333333],\n",
       " [0.15831578947368422, 0.10015789473684211, 0.12989473684210526],\n",
       " [0.15564, 0.08512000000000003, 0.10851999999999999],\n",
       " [0.08115789473684211, 0.09947368421052633, 0.09610526315789474],\n",
       " [0.09884210526315788, 0.08463157894736843, 0.08326315789473683],\n",
       " [0.0998125, 0.1169375, 0.10893749999999999],\n",
       " [0.1223529411764706, 0.10670588235294118, 0.12758823529411764],\n",
       " [0.2690666666666667, 0.20793333333333333, 0.2552],\n",
       " [0.24154545454545454, 0.1948181818181818, 0.22363636363636363],\n",
       " [0.26871428571428574, 0.1677857142857143, 0.2384285714285714],\n",
       " [0.1111111111111111, 0.1031111111111111, 0.11666666666666667],\n",
       " [0.2480416666666667, 0.2047083333333333, 0.177875],\n",
       " [0.09585714285714285, 0.09171428571428572, 0.08542857142857142],\n",
       " [0.07859999999999999, 0.040799999999999996, 0.0618],\n",
       " [0.19096875, 0.13015624999999997, 0.17503124999999997],\n",
       " [0.17031818181818184, 0.11995454545454544, 0.1596818181818182],\n",
       " [0.1698846153846154, 0.12257692307692307, 0.15573076923076926],\n",
       " [0.12181818181818183, 0.09963636363636362, 0.09745454545454546],\n",
       " [0.06999999999999999, 0.046700000000000005, 0.0746],\n",
       " [0.05833333333333333, 0.03891666666666667, 0.06216666666666667],\n",
       " [0.2422941176470588, 0.1656470588235294, 0.20017647058823532],\n",
       " [0.06244444444444445, 0.03911111111111112, 0.03766666666666666],\n",
       " [0.11678947368421051, 0.09663157894736843, 0.11442105263157894],\n",
       " [0.1408695652173913, 0.10352173913043479, 0.12434782608695652],\n",
       " [0.10234782608695653, 0.08560869565217391, 0.09378260869565218],\n",
       " [0.0514, 0.07146666666666666, 0.07546666666666667],\n",
       " [0.17629166666666665, 0.1685, 0.14925],\n",
       " [0.18038709677419354, 0.1522903225806452, 0.15432258064516127],\n",
       " [0.21243749999999997, 0.13693750000000002, 0.18640625],\n",
       " [0.20729999999999998, 0.11000000000000001, 0.17540000000000003],\n",
       " [0.14533333333333334, 0.08211111111111111, 0.11666666666666667],\n",
       " [0.151625, 0.08875, 0.113875],\n",
       " [0.11480769230769232, 0.06346153846153846, 0.0915],\n",
       " [0.12780952380952385, 0.06614285714285716, 0.1139047619047619],\n",
       " [0.15221212121212122, 0.08409090909090909, 0.1346969696969697],\n",
       " [0.11891666666666667, 0.07808333333333332, 0.09408333333333334],\n",
       " [0.15855555555555556, 0.1041111111111111, 0.12544444444444444],\n",
       " [0.15855555555555556, 0.1041111111111111, 0.12544444444444444],\n",
       " [0.14061111111111113, 0.07094444444444443, 0.097],\n",
       " [0.14672727272727273, 0.06254545454545454, 0.10636363636363635],\n",
       " [0.14485714285714285, 0.09221428571428571, 0.1052142857142857],\n",
       " [0.22430769230769235, 0.16530769230769235, 0.17615384615384616],\n",
       " [0.16328571428571428, 0.08785714285714286, 0.13342857142857142],\n",
       " [0.1674, 0.10399999999999998, 0.1441],\n",
       " [0.17273333333333335, 0.09163333333333332, 0.11589999999999999],\n",
       " [0.15940740740740741, 0.07540740740740741, 0.10814814814814815],\n",
       " [0.0379375, 0.024249999999999997, 0.0451875],\n",
       " [0.1059090909090909, 0.08933333333333335, 0.1073030303030303],\n",
       " [0.18873684210526318, 0.10026315789473685, 0.15563157894736843],\n",
       " [0.2161818181818182, 0.1571818181818182, 0.18181818181818182],\n",
       " [0.24005, 0.1936, 0.18325],\n",
       " [0.24586363636363637, 0.19645454545454547, 0.18199999999999997],\n",
       " [0.087, 0.08272727272727273, 0.07195454545454547],\n",
       " [0.14709090909090908, 0.06654545454545455, 0.0870909090909091],\n",
       " [0.12955555555555553, 0.07877777777777778, 0.08566666666666665],\n",
       " [0.154625, 0.073875, 0.12037500000000001],\n",
       " [0.18549999999999997, 0.10983333333333334, 0.14905555555555555],\n",
       " [0.14842307692307694, 0.09242307692307693, 0.12269230769230771],\n",
       " [0.16063157894736846, 0.16942105263157897, 0.157],\n",
       " [0.1622222222222222, 0.15455555555555558, 0.15485185185185185],\n",
       " [0.12238888888888891, 0.09755555555555555, 0.10222222222222221],\n",
       " [0.12379166666666665, 0.11445833333333334, 0.11291666666666667],\n",
       " [0.17286842105263156, 0.14478947368421055, 0.1496315789473684],\n",
       " [0.2116470588235294, 0.16205882352941176, 0.1750588235294118],\n",
       " [0.17951612903225808, 0.13199999999999998, 0.16158064516129036],\n",
       " [0.15434375, 0.14121875, 0.14184375000000002],\n",
       " [0.17712820512820512, 0.1473846153846154, 0.15482051282051282],\n",
       " [0.14064285714285713, 0.08778571428571429, 0.10707142857142858],\n",
       " [0.25253846153846154, 0.16257692307692306, 0.20142307692307693],\n",
       " [0.1022, 0.0705, 0.08145],\n",
       " [0.3179, 0.2366, 0.2638],\n",
       " [0.1675, 0.13699999999999998, 0.134],\n",
       " [0.23330952380952383, 0.12104761904761904, 0.15954761904761905],\n",
       " [0.19462500000000002, 0.12308333333333332, 0.13833333333333334],\n",
       " [0.19929411764705882, 0.12911764705882356, 0.1438235294117647],\n",
       " [0.16572727272727272, 0.1318181818181818, 0.1319090909090909],\n",
       " [0.18855555555555556, 0.16044444444444445, 0.1531111111111111],\n",
       " [0.21695348837209305, 0.15088372093023256, 0.16788372093023254],\n",
       " [0.2166829268292683, 0.13597560975609757, 0.1624390243902439],\n",
       " [0.24225, 0.110875, 0.15087499999999998],\n",
       " [0.10422222222222222, 0.04088888888888889, 0.059333333333333335],\n",
       " [0.0625, 0.056875, 0.077375],\n",
       " [0.20818749999999997, 0.1418125, 0.16162500000000002],\n",
       " [0.20382142857142854, 0.13789285714285712, 0.16035714285714286],\n",
       " [0.20606666666666668, 0.13153333333333334, 0.1998],\n",
       " [0.240125, 0.14516666666666667, 0.20775],\n",
       " [0.28126666666666666, 0.15593333333333337, 0.19413333333333332],\n",
       " [0.21000000000000002, 0.15655, 0.16535],\n",
       " [0.03775, 0.0885, 0.077],\n",
       " [0.16976923076923076, 0.19484615384615384, 0.18215384615384617],\n",
       " [0.22596969696969696, 0.14993939393939393, 0.1956060606060606],\n",
       " [0.22670588235294117, 0.15135294117647058, 0.15905882352941178],\n",
       " [0.14595652173913046, 0.12278260869565216, 0.1048695652173913],\n",
       " [0.17904347826086958, 0.12978260869565217, 0.15265217391304348],\n",
       " [0.2005, 0.1283181818181818, 0.11045454545454544],\n",
       " [0.14018181818181816, 0.10945454545454546, 0.07036363636363636],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.11015384615384614, 0.0663076923076923, 0.10153846153846155],\n",
       " [0.182, 0.12379310344827588, 0.1589310344827586],\n",
       " [0.2005142857142857, 0.15968571428571426, 0.16802857142857144],\n",
       " [0.15860000000000002, 0.14304999999999998, 0.1296],\n",
       " [0.22958333333333336, 0.16216666666666665, 0.15574999999999997],\n",
       " [0.195875, 0.15912500000000002, 0.18362499999999998],\n",
       " [0.15833333333333333, 0.15291666666666667, 0.14175],\n",
       " [0.20852631578947367, 0.15594736842105264, 0.19231578947368422],\n",
       " [0.23138461538461538, 0.18969230769230772, 0.19276923076923078],\n",
       " [0.08335294117647057, 0.051, 0.07088235294117648],\n",
       " [0.1266, 0.10979999999999998, 0.10754999999999999],\n",
       " [0.11850000000000001, 0.0654, 0.09104999999999999],\n",
       " [0.06858333333333333, 0.02, 0.05391666666666667],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0719, 0.0353, 0.0446],\n",
       " [0.16729411764705882, 0.11435294117647059, 0.1338235294117647],\n",
       " [0.14788372093023255, 0.08890697674418603, 0.1267906976744186],\n",
       " [0.0941304347826087, 0.06291304347826088, 0.08491304347826087],\n",
       " [0.11708333333333333, 0.08233333333333333, 0.09325],\n",
       " [0.07633333333333334, 0.04722222222222222, 0.0645],\n",
       " [0.351375, 0.26825, 0.228375],\n",
       " [0.31758333333333333, 0.22208333333333333, 0.20833333333333334],\n",
       " [0.30383333333333334, 0.28583333333333333, 0.18483333333333332],\n",
       " [0.5208571428571428, 0.48999999999999994, 0.31685714285714284],\n",
       " [0.19118181818181815, 0.20227272727272724, 0.14536363636363636],\n",
       " [0.11460000000000001, 0.06673333333333333, 0.0888],\n",
       " [0.14833333333333334, 0.07504761904761906, 0.10571428571428572],\n",
       " [0.1156153846153846, 0.06561538461538462, 0.09680769230769232],\n",
       " [0.147, 0.09246153846153846, 0.12323076923076923],\n",
       " [0.19015625000000003, 0.1085, 0.17221875],\n",
       " [0.12216666666666669, 0.07347222222222222, 0.11186111111111112],\n",
       " [0.15334782608695652, 0.14691304347826087, 0.14369565217391306],\n",
       " [0.002, 0.09179999999999999, 0.056499999999999995],\n",
       " [0.2046, 0.14946666666666666, 0.1446],\n",
       " [0.08959999999999999, 0.0765, 0.0673],\n",
       " [0.13063636363636363, 0.06281818181818183, 0.07445454545454545],\n",
       " [0.09377777777777778, 0.12066666666666664, 0.1101111111111111],\n",
       " [0.08911111111111111, 0.02333333333333333, 0.06733333333333333],\n",
       " [0.20950000000000002, 0.09672222222222222, 0.16861111111111113],\n",
       " [0.11094117647058822, 0.08382352941176471, 0.08988235294117647],\n",
       " [0.1656875, 0.13, 0.17743749999999997],\n",
       " [0.10366666666666667, 0.10416666666666667, 0.07616666666666667],\n",
       " [0.09050000000000001, 0.03375, 0.09912499999999999],\n",
       " [0.010333333333333333, 0.12883333333333333, 0.04850000000000001],\n",
       " [0.26642857142857146, 0.15457142857142855, 0.18892857142857142],\n",
       " [0.12541666666666665, 0.05941666666666667, 0.11783333333333335],\n",
       " [0.06549999999999999, 0.033999999999999996, 0.0515],\n",
       " [0.19809090909090907, 0.13863636363636364, 0.17936363636363636],\n",
       " [0.10086363636363636, 0.13609090909090907, 0.1509090909090909],\n",
       " [0.15331818181818183, 0.08440909090909092, 0.11018181818181817],\n",
       " [0.13090000000000002, 0.07070000000000001, 0.08650000000000001],\n",
       " [0.066375, 0.03875, 0.04125],\n",
       " [0.11033333333333332, 0.10091666666666665, 0.09325],\n",
       " [0.12033333333333335, 0.125, 0.11341666666666667],\n",
       " [0.28005882352941175, 0.15976470588235295, 0.2083529411764706],\n",
       " [0.16975, 0.17149999999999999, 0.131125],\n",
       " [0.14361904761904762, 0.07942857142857143, 0.11476190476190477],\n",
       " [0.102, 0.08866666666666667, 0.11583333333333333],\n",
       " [0.14275, 0.1255, 0.156625],\n",
       " [0.19638095238095238, 0.1352857142857143, 0.14219047619047617],\n",
       " [0.23727777777777778, 0.1248888888888889, 0.17],\n",
       " [0.10714285714285714, 0.04942857142857143, 0.07142857142857142],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.12440000000000002, 0.08154285714285715, 0.09414285714285714],\n",
       " [0.18936363636363637, 0.11372727272727272, 0.13281818181818184],\n",
       " [0.18987500000000002, 0.1776875, 0.169375],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.2317222222222222, 0.13361111111111112, 0.17127777777777775],\n",
       " [0.22938095238095235, 0.1331904761904762, 0.16799999999999998],\n",
       " [0.1426538461538462, 0.1195, 0.14365384615384616],\n",
       " [0.13317391304347828, 0.11804347826086956, 0.14304347826086958],\n",
       " [0.21706666666666669, 0.20719999999999997, 0.2134],\n",
       " [0.21657142857142858, 0.231, 0.20123809523809524],\n",
       " [0.09938461538461539, 0.1340769230769231, 0.07884615384615384],\n",
       " [0.16411538461538464, 0.09526923076923076, 0.1244230769230769],\n",
       " [0.19765517241379313, 0.17320689655172414, 0.18444827586206894],\n",
       " [0.16, 0.103625, 0.1365],\n",
       " [0.18334782608695654, 0.08530434782608695, 0.11343478260869565],\n",
       " [0.19171428571428573, 0.17757142857142855, 0.1377857142857143],\n",
       " [0.07229166666666667, 0.04045833333333334, 0.06733333333333334],\n",
       " [0.06989473684210526, 0.031052631578947366, 0.06426315789473684],\n",
       " [0.20307692307692304, 0.1696153846153846, 0.15484615384615383],\n",
       " [0.09505555555555557, 0.042055555555555554, 0.06183333333333333],\n",
       " [0.10427272727272728, 0.057272727272727274, 0.08618181818181818],\n",
       " [0.16066666666666668, 0.15583333333333335, 0.17608333333333334],\n",
       " [0.1792083333333333, 0.12979166666666667, 0.16445833333333335],\n",
       " [0.22608333333333333, 0.09175, 0.12691666666666668],\n",
       " [0.111375, 0.0883125, 0.084875],\n",
       " [0.15025, 0.115375, 0.11375],\n",
       " [0.2485714285714286, 0.1437142857142857, 0.21228571428571433],\n",
       " [0.06945454545454545, 0.09186363636363636, 0.06704545454545455],\n",
       " [0.31633333333333336, 0.17866666666666667, 0.22899999999999998],\n",
       " [0.1855294117647059, 0.1381176470588235, 0.1816470588235294],\n",
       " [0.22949999999999998, 0.11800000000000001, 0.11525],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.18854545454545454, 0.13690909090909092, 0.16218181818181818],\n",
       " [0.1628125, 0.10756250000000002, 0.134625],\n",
       " [0.0635625, 0.1165625, 0.09475],\n",
       " [0.06664285714285714, 0.08592857142857142, 0.07385714285714286],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.08375, 0.06849999999999999, 0.067],\n",
       " [0.15628571428571428, 0.15892857142857145, 0.15757142857142856],\n",
       " [0.10376470588235293, 0.1193529411764706, 0.10611764705882354],\n",
       " [0.10081481481481483, 0.06155555555555556, 0.09092592592592592],\n",
       " [0.1272222222222222, 0.08177777777777778, 0.1231111111111111],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.17374074074074072, 0.11125925925925927, 0.1521111111111111],\n",
       " [0.2718333333333333, 0.17526190476190476, 0.22961904761904758],\n",
       " [0.1713529411764706, 0.1413529411764706, 0.15711764705882356],\n",
       " [0.09744827586206899, 0.08882758620689656, 0.09803448275862069],\n",
       " [0.27573333333333333, 0.19046666666666665, 0.2792],\n",
       " [0.173811320754717, 0.12254716981132076, 0.13232075471698113],\n",
       " [0.21962500000000004, 0.13708333333333333, 0.18045833333333336],\n",
       " [0.22676190476190475, 0.17238095238095238, 0.19052380952380954],\n",
       " [0.217, 0.1781111111111111, 0.19566666666666668],\n",
       " [0.19246153846153843, 0.14553846153846153, 0.12423076923076921],\n",
       " [0.17981818181818182, 0.145, 0.1284090909090909],\n",
       " [0.16558333333333333, 0.12141666666666666, 0.15075000000000002],\n",
       " [0.2312857142857143, 0.18385714285714286, 0.20633333333333334],\n",
       " [0.19864864864864867, 0.1434864864864865, 0.16383783783783784],\n",
       " [0.09972000000000002, 0.05792, 0.06916],\n",
       " [0.23589655172413798, 0.18448275862068966, 0.20210344827586213],\n",
       " [0.20824999999999996, 0.14135714285714282, 0.1634642857142857],\n",
       " [0.099, 0.0428, 0.08349999999999999],\n",
       " [0.18060714285714288, 0.17082142857142854, 0.1529642857142857],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.19731818181818184, 0.15425, 0.17388636363636364],\n",
       " [0.15044444444444446, 0.14092592592592593, 0.13633333333333333],\n",
       " [0.19517073170731708, 0.15753658536585366, 0.1775853658536585],\n",
       " [0.15677272727272729, 0.08345454545454546, 0.09372727272727271],\n",
       " [0.08478947368421054, 0.04657894736842105, 0.08573684210526315],\n",
       " [0.15014285714285713, 0.17142857142857143, 0.13014285714285714],\n",
       " [0.15078571428571427, 0.11299999999999999, 0.12964285714285714],\n",
       " [0.14688235294117646, 0.07305882352941176, 0.10811764705882354],\n",
       " [0.19040909090909092, 0.08081818181818182, 0.13036363636363638],\n",
       " [0.2430476190476191, 0.10195238095238095, 0.1605238095238095],\n",
       " [0.139, 0.07852173913043478, 0.10265217391304349],\n",
       " [0.08033333333333334, 0.0402, 0.06453333333333333],\n",
       " [0.14272727272727273, 0.07381818181818181, 0.13736363636363635],\n",
       " [0.14478787878787877, 0.08751515151515153, 0.11472727272727272],\n",
       " [0.2004705882352941, 0.11429411764705882, 0.14585294117647057],\n",
       " [0.2216428571428571, 0.1363214285714286, 0.17157142857142857],\n",
       " [0.095, 0.0748695652173913, 0.08939130434782609],\n",
       " [0.09115384615384615, 0.07592307692307693, 0.09876923076923078],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.37678947368421045, 0.21784210526315786, 0.28405263157894733],\n",
       " [0.08066666666666666, 0.13683333333333333, 0.11950000000000001],\n",
       " [0.06085, 0.0252, 0.0501],\n",
       " [0.147, 0.10073333333333334, 0.11173333333333332],\n",
       " [0.1824, 0.16, 0.16620000000000001],\n",
       " [0.2211, 0.1197, 0.14766666666666667],\n",
       " [0.2118636363636364, 0.09654545454545455, 0.1469090909090909],\n",
       " [0.12130769230769231, 0.08346153846153848, 0.08676923076923078],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.06271428571428571, 0.024857142857142855, 0.04314285714285714],\n",
       " [0.1464, 0.1592, 0.1693],\n",
       " [0.16013333333333335, 0.13066666666666665, 0.1484666666666667],\n",
       " [0.13904999999999998, 0.08585000000000001, 0.10235000000000001],\n",
       " [0.1151875, 0.08431250000000001, 0.0945625],\n",
       " [0.1887777777777778, 0.11966666666666666, 0.1622222222222222],\n",
       " [0.12135714285714286, 0.07692857142857143, 0.10428571428571429],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1782608695652174, 0.08460869565217391, 0.14530434782608695],\n",
       " [0.05338461538461539, 0.04161538461538462, 0.05515384615384616],\n",
       " [0.10438095238095237, 0.11604761904761907, 0.11338095238095237],\n",
       " [0.0635, 0.026899999999999997, 0.0312],\n",
       " [0.13785294117647057, 0.09508823529411765, 0.10394117647058823],\n",
       " [0.17454838709677417, 0.113, 0.13958064516129032],\n",
       " [0.06033333333333334, 0.022500000000000003, 0.06608333333333333],\n",
       " [0.09546666666666666, 0.05746666666666666, 0.08800000000000001],\n",
       " [0.3126875, 0.16031250000000002, 0.2755],\n",
       " [0.25541176470588234, 0.12135294117647057, 0.2078235294117647],\n",
       " [0.07009090909090908, 0.03272727272727272, 0.04045454545454546],\n",
       " [0.152125, 0.1245, 0.13443750000000002],\n",
       " [0.18284615384615385, 0.12025641025641025, 0.14453846153846156],\n",
       " [0.14877142857142855, 0.08968571428571427, 0.11228571428571428],\n",
       " [0.09955555555555556, 0.07816666666666666, 0.09333333333333334],\n",
       " [0.110625, 0.052750000000000005, 0.092625],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.17341379310344826, 0.11034482758620688, 0.13137931034482758],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.2347857142857143, 0.12014285714285715, 0.20521428571428574],\n",
       " [0.12084, 0.057800000000000004, 0.09788000000000001],\n",
       " [0.005928571428571429, 0.04928571428571428, 0.018142857142857145],\n",
       " [0.1036, 0.088, 0.074],\n",
       " [0.09463636363636363, 0.07627272727272727, 0.08418181818181819],\n",
       " [0.081, 0.047437499999999994, 0.07412500000000001],\n",
       " [0.0765, 0.0361, 0.0704],\n",
       " [0.1780454545454546, 0.10936363636363634, 0.12436363636363637],\n",
       " [0.1498918918918919, 0.10886486486486485, 0.11345945945945947],\n",
       " [0.10524242424242423, 0.09242424242424242, 0.09212121212121212],\n",
       " [0.10244444444444444, 0.07933333333333333, 0.08766666666666666],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.06343478260869566, 0.03447826086956522, 0.05378260869565218],\n",
       " [0.16158333333333333, 0.10020833333333334, 0.12466666666666666],\n",
       " [0.21439999999999998, 0.1446, 0.18255],\n",
       " [0.16557142857142856, 0.11789795918367349, 0.1324081632653061],\n",
       " [0.1495, 0.12725, 0.133],\n",
       " [0.0783888888888889, 0.04527777777777778, 0.07811111111111112],\n",
       " [0.029933333333333333, 0.0148, 0.017333333333333333],\n",
       " [0.1541904761904762, 0.1296190476190476, 0.13138095238095238],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0872857142857143, 0.0540952380952381, 0.0790952380952381],\n",
       " [0.31227272727272726, 0.20336363636363639, 0.1912727272727273],\n",
       " [0.26535000000000003, 0.253, 0.18440000000000004],\n",
       " [0.08986206896551725, 0.05072413793103449, 0.06493103448275862],\n",
       " [0.10442424242424242, 0.08175757575757575, 0.08984848484848484],\n",
       " [0.17309999999999998, 0.1246, 0.159],\n",
       " [0.169, 0.09407692307692307, 0.15569230769230769],\n",
       " [0.09228571428571428, 0.06485714285714285, 0.1002857142857143],\n",
       " [0.179375, 0.11362499999999999, 0.15500000000000003],\n",
       " [0.11958333333333333, 0.07575, 0.10333333333333335],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.17303703703703702, 0.0992962962962963, 0.14240740740740743],\n",
       " [0.2279047619047619, 0.11299999999999999, 0.17371428571428568],\n",
       " [0.13111111111111112, 0.052185185185185196, 0.08822222222222222],\n",
       " [0.0822857142857143, 0.03152380952380952, 0.05733333333333334],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1133125, 0.0806875, 0.091375],\n",
       " [0.20884210526315788, 0.15500000000000003, 0.1792631578947368],\n",
       " [0.1725, 0.06125, 0.08975],\n",
       " [0.20525, 0.12685, 0.1517],\n",
       " [0.17200000000000001, 0.11750000000000001, 0.1530625],\n",
       " [0.16249999999999998, 0.11372222222222222, 0.14016666666666666],\n",
       " [0.25499999999999995, 0.1328695652173913, 0.17300000000000001],\n",
       " [0.23295454545454541, 0.1119090909090909, 0.15054545454545454],\n",
       " [0.13152380952380954, 0.07728571428571428, 0.08595238095238096],\n",
       " [0.12424000000000002, 0.08963999999999998, 0.08860000000000001],\n",
       " [0.22458823529411764, 0.18511764705882353, 0.19305882352941175],\n",
       " [0.34740000000000004, 0.2529, 0.2872],\n",
       " [0.22750000000000004, 0.1149, 0.1712],\n",
       " [0.20681818181818185, 0.10445454545454545, 0.15563636363636363],\n",
       " [0.20145, 0.16099999999999998, 0.19380000000000003],\n",
       " [0.196, 0.16926086956521738, 0.18408695652173912],\n",
       " [0.0958, 0.1346, 0.0716],\n",
       " [0.16064285714285712, 0.14628571428571427, 0.1345],\n",
       " [0.126, 0.09280000000000001, 0.10024999999999999],\n",
       " [0.08911764705882354, 0.049529411764705884, 0.06964705882352942],\n",
       " [0.13835714285714287, 0.09710714285714285, 0.11239285714285714],\n",
       " [0.13762857142857143, 0.10434285714285714, 0.1054],\n",
       " [0.202, 0.128, 0.16326923076923078],\n",
       " [0.15711999999999998, 0.15208, 0.14687999999999998],\n",
       " [0.1278, 0.08449999999999999, 0.11179999999999998],\n",
       " [0.16466666666666666, 0.12125000000000001, 0.14774999999999996],\n",
       " [0.18110344827586206, 0.11524137931034485, 0.1510344827586207],\n",
       " [0.16264285714285714, 0.09757142857142857, 0.13303571428571428],\n",
       " [0.17284615384615384, 0.0936153846153846, 0.13530769230769232],\n",
       " [0.1687777777777778, 0.09588888888888888, 0.12177777777777779],\n",
       " [0.13466666666666668, 0.10741666666666667, 0.12675],\n",
       " [0.1343333333333333, 0.1312666666666667, 0.1273],\n",
       " [0.1719166666666667, 0.10716666666666667, 0.12716666666666668],\n",
       " [0.041666666666666664, 0.04422222222222222, 0.035666666666666666],\n",
       " [0.1679166666666667, 0.11291666666666667, 0.15125000000000002],\n",
       " [0.09525, 0.05275, 0.06125],\n",
       " [0.07375000000000001, 0.03125, 0.0539375],\n",
       " [0.1443030303030303, 0.08596969696969697, 0.10906060606060607],\n",
       " [0.11273684210526318, 0.15205263157894738, 0.11726315789473683],\n",
       " [0.05692307692307692, 0.02707692307692308, 0.054615384615384614],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.09722222222222222, 0.08911111111111111, 0.07133333333333333],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1371764705882353, 0.09629411764705882, 0.08652941176470588],\n",
       " [0.18933333333333333, 0.11804761904761905, 0.17147619047619048],\n",
       " [0.23399999999999999, 0.13278571428571428, 0.12342857142857144],\n",
       " [0.17958823529411763, 0.09929411764705882, 0.15788235294117647],\n",
       " [0.1585, 0.07100000000000001, 0.13355],\n",
       " [0.13915151515151516, 0.11445454545454545, 0.1225151515151515],\n",
       " [0.15803571428571428, 0.15978571428571428, 0.1430357142857143],\n",
       " [0.14411111111111113, 0.14655555555555555, 0.16277777777777777],\n",
       " [0.10342857142857144, 0.038571428571428576, 0.11328571428571428],\n",
       " [0.2475, 0.13724999999999998, 0.14],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.09475, 0.150625, 0.122875],\n",
       " [0.10828571428571429, 0.17214285714285715, 0.14042857142857143],\n",
       " [0.25157894736842107, 0.21136842105263157, 0.22799999999999998],\n",
       " [0.08365517241379311, 0.08662068965517242, 0.09568965517241379],\n",
       " [0.15647619047619046, 0.10914285714285714, 0.11561904761904764],\n",
       " [0.25414285714285717, 0.138, 0.21814285714285714],\n",
       " [0.24452173913043482, 0.16782608695652176, 0.20865217391304344],\n",
       " [0.2029285714285714, 0.1682142857142857, 0.16478571428571429],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.087, 0.07245, 0.07115],\n",
       " [0.1180952380952381, 0.08576190476190478, 0.10157142857142858],\n",
       " [0.148, 0.0704, 0.142],\n",
       " [0.19008695652173913, 0.1400869565217391, 0.19334782608695653],\n",
       " [0.13705555555555554, 0.07566666666666666, 0.10488888888888888],\n",
       " [0.13705555555555554, 0.07566666666666666, 0.10488888888888888],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.3007272727272727, 0.17931818181818182, 0.21768181818181817],\n",
       " [0.12942857142857142, 0.11957142857142858, 0.1212857142857143],\n",
       " [0.09259090909090908, 0.05018181818181819, 0.053],\n",
       " [0.13784848484848483, 0.08212121212121212, 0.10818181818181818],\n",
       " [0.143375, 0.08816666666666667, 0.12620833333333334],\n",
       " [0.14625, 0.0828125, 0.1269375],\n",
       " [0.2166857142857143, 0.13508571428571425, 0.16945714285714286],\n",
       " [0.2214285714285714, 0.13766666666666666, 0.17638095238095236],\n",
       " [0.22157894736842104, 0.12757894736842104, 0.1623157894736842],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.11166666666666668, 0.04, 0.09233333333333332],\n",
       " [0.09571428571428572, 0.03428571428571429, 0.07914285714285714],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.14520833333333333, 0.09154166666666667, 0.12737500000000002],\n",
       " [0.2106666666666667, 0.09483333333333333, 0.13833333333333334],\n",
       " [0.2720526315789474, 0.18221052631578952, 0.17326315789473684],\n",
       " [0.07137500000000001, 0.03875, 0.060250000000000005],\n",
       " [0.13312499999999997, 0.10262500000000001, 0.11033333333333334],\n",
       " [0.05030769230769231, 0.06876923076923076, 0.07161538461538461],\n",
       " [0.13499999999999998, 0.10733333333333332, 0.12746666666666667],\n",
       " [0.14273333333333332, 0.10653333333333333, 0.14340000000000003],\n",
       " [0.06999999999999999, 0.032705882352941175, 0.057058823529411766],\n",
       " [0.10818181818181817, 0.05054545454545454, 0.08818181818181818],\n",
       " [0.10853846153846154, 0.048692307692307694, 0.07015384615384615],\n",
       " [0.083, 0.03723529411764706, 0.053647058823529416],\n",
       " [0.16444444444444445, 0.07822222222222223, 0.15777777777777777],\n",
       " [0.22971999999999998, 0.14064, 0.19891999999999999],\n",
       " [0.1361923076923077, 0.0821153846153846, 0.11976923076923077],\n",
       " [0.11868181818181817, 0.07695454545454544, 0.11199999999999997],\n",
       " [0.054285714285714284, 0.030928571428571427, 0.04235714285714286],\n",
       " [0.054285714285714284, 0.030928571428571427, 0.04235714285714286],\n",
       " [0.12333333333333334, 0.05866666666666667, 0.11833333333333333],\n",
       " [0.2282727272727273, 0.10918181818181819, 0.17345454545454544],\n",
       " [0.16154166666666667, 0.11279166666666668, 0.14404166666666665],\n",
       " [0.19049999999999997, 0.09864285714285716, 0.15785714285714286],\n",
       " [0.17716666666666667, 0.13410000000000002, 0.15856666666666666],\n",
       " [0.14936666666666668, 0.12576666666666667, 0.13463333333333333],\n",
       " [0.1941818181818182, 0.13154545454545455, 0.1490909090909091],\n",
       " [0.18957142857142856, 0.1457142857142857, 0.13571428571428573],\n",
       " [0.24907142857142858, 0.1585, 0.22249999999999998],\n",
       " [0.041666666666666664, 0.03575, 0.03216666666666667],\n",
       " [0.21121875, 0.11134375, 0.13912500000000003],\n",
       " [0.25325000000000003, 0.20199999999999999, 0.24950000000000003],\n",
       " [0.20672413793103447, 0.12720689655172418, 0.1526206896551724],\n",
       " [0.26749999999999996, 0.24828571428571428, 0.21964285714285717],\n",
       " [0.13982758620689656, 0.09320689655172414, 0.11865517241379311],\n",
       " [0.13823076923076924, 0.07615384615384616, 0.09549999999999999],\n",
       " [0.19199999999999998, 0.09142857142857144, 0.18842857142857142],\n",
       " [0.035687500000000004, 0.0165, 0.0325625],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.048125, 0.08725, 0.054000000000000006],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.09726666666666667, 0.08006666666666667, 0.09119999999999999],\n",
       " [0.22064705882352942, 0.15676470588235294, 0.19052941176470586],\n",
       " [0.2096470588235294, 0.15100000000000002, 0.19523529411764706],\n",
       " [0.15628571428571428, 0.1335714285714286, 0.13428571428571429],\n",
       " [0.12503703703703703, 0.1107037037037037, 0.1138148148148148],\n",
       " [0.17471428571428574, 0.12907142857142856, 0.13757142857142857],\n",
       " [0.242, 0.13380952380952382, 0.19328571428571428],\n",
       " [0.09209090909090908, 0.07718181818181818, 0.08609090909090908],\n",
       " [0.1457391304347826, 0.10830434782608696, 0.1137391304347826],\n",
       " [0.16057894736842107, 0.11352631578947368, 0.10784210526315786],\n",
       " [0.13642105263157894, 0.10157894736842105, 0.10084210526315789],\n",
       " [0.09809999999999999, 0.0664, 0.07909999999999999],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.2662222222222222, 0.16922222222222225, 0.22666666666666668],\n",
       " [0.07444444444444445, 0.06088888888888888, 0.059555555555555556],\n",
       " [0.09473913043478262, 0.051956521739130436, 0.06],\n",
       " [0.18952631578947365, 0.14226315789473684, 0.1726315789473684],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.12333333333333334, 0.05866666666666667, 0.11833333333333333],\n",
       " [0.16149999999999998, 0.13225, 0.13875],\n",
       " [0.16364705882352942, 0.17058823529411765, 0.15447058823529414],\n",
       " [0.1982972972972973, 0.1508918918918919, 0.17481081081081085],\n",
       " [0.0948, 0.08399999999999999, 0.054700000000000006],\n",
       " [0.1451578947368421, 0.09742105263157895, 0.1],\n",
       " [0.12538709677419357, 0.0762258064516129, 0.09983870967741934],\n",
       " [0.13704166666666664, 0.09099999999999998, 0.11220833333333334],\n",
       " [0.2203636363636364, 0.15127272727272725, 0.17290909090909093],\n",
       " [0.14, 0.08727777777777776, 0.11116666666666666],\n",
       " [0.10907142857142858, 0.0895, 0.08578571428571428],\n",
       " [0.10270588235294117, 0.10394117647058823, 0.07952941176470588],\n",
       " [0.1524375, 0.08606249999999999, 0.1130625],\n",
       " [0.1524375, 0.08606249999999999, 0.1130625],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.10177777777777777, 0.037, 0.06533333333333333],\n",
       " [0.16067857142857142, 0.11635714285714287, 0.1343214285714286],\n",
       " [0.07137500000000001, 0.03875, 0.060250000000000005],\n",
       " [0.25552, 0.13692000000000001, 0.18752000000000002],\n",
       " [0.2487272727272727, 0.10018181818181819, 0.14536363636363636],\n",
       " [0.10557894736842104, 0.0933157894736842, 0.09321052631578947],\n",
       " [0.14200000000000002, 0.10181481481481482, 0.12066666666666664],\n",
       " [0.09905882352941177, 0.10599999999999998, 0.08523529411764706],\n",
       " [0.1562, 0.1002, 0.14251999999999998],\n",
       " [0.2375238095238095, 0.11561904761904762, 0.20633333333333334],\n",
       " [0.1880666666666667, 0.2602666666666667, 0.18106666666666668],\n",
       " [0.06615384615384615, 0.07126923076923077, 0.05342307692307692],\n",
       " [0.21303571428571427, 0.16478571428571429, 0.1788214285714286],\n",
       " [0.22515384615384615, 0.12953846153846155, 0.19892307692307695],\n",
       " [0.0625, 0.053799999999999994, 0.053000000000000005],\n",
       " [0.06969230769230769, 0.06438461538461539, 0.06530769230769232],\n",
       " [0.17452173913043478, 0.14430434782608695, 0.12886956521739132],\n",
       " [0.23320000000000002, 0.16875, 0.20549999999999996],\n",
       " [0.16017647058823528, 0.1144705882352941, 0.1373529411764706],\n",
       " [0.166625, 0.107375, 0.094125],\n",
       " [0.05555555555555555, 0.050555555555555555, 0.06877777777777777],\n",
       " [0.13727777777777778, 0.08027777777777778, 0.11983333333333333],\n",
       " [0.12405, 0.07155, 0.09910000000000001],\n",
       " [0.021666666666666667, 0.12083333333333333, 0.04533333333333334],\n",
       " [0.13813333333333333, 0.13519999999999996, 0.13573333333333334],\n",
       " [0.17825000000000002, 0.11454166666666667, 0.15874999999999997],\n",
       " [0.22541666666666668, 0.14541666666666667, 0.18533333333333335],\n",
       " [0.28784000000000004, 0.19148000000000004, 0.23520000000000002],\n",
       " [0.2538888888888889, 0.16849999999999998, 0.21344444444444444],\n",
       " [0.09972727272727272, 0.08372727272727272, 0.07663636363636363],\n",
       " [0.2310714285714286, 0.11621428571428573, 0.1494285714285714],\n",
       " [0.095625, 0.045125, 0.08800000000000001],\n",
       " [0.140625, 0.085125, 0.1225],\n",
       " [0.1355, 0.073, 0.0925],\n",
       " [0.09033333333333333, 0.048666666666666664, 0.06166666666666667],\n",
       " [0.12828571428571428, 0.07928571428571429, 0.09485714285714286],\n",
       " [0.1796, 0.11100000000000002, 0.1328],\n",
       " [0.07142857142857142, 0.07685714285714285, 0.07271428571428572],\n",
       " [0.25670833333333337, 0.1535, 0.2374166666666667],\n",
       " [0.06053333333333333, 0.062066666666666666, 0.04726666666666667],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.19125, 0.09025, 0.17600000000000002],\n",
       " [0.16014285714285714, 0.1178095238095238, 0.1327142857142857],\n",
       " [0.12111538461538464, 0.09473076923076923, 0.09769230769230769],\n",
       " [0.14736666666666667, 0.11096666666666666, 0.14176666666666668],\n",
       " [0.14333333333333334, 0.11033333333333334, 0.14059259259259257],\n",
       " [0.0925, 0.044000000000000004, 0.08875],\n",
       " [0.14695238095238097, 0.07890476190476191, 0.10276190476190478],\n",
       " [0.09293548387096776, 0.07154838709677419, 0.07609677419354839],\n",
       " [0.17563636363636362, 0.1410909090909091, 0.12581818181818183],\n",
       " [0.24533333333333332, 0.19166666666666665, 0.18866666666666668],\n",
       " [0.24550000000000002, 0.18834615384615386, 0.2378076923076923],\n",
       " [0.19643999999999998, 0.14987999999999999, 0.20204],\n",
       " [0.1369, 0.046099999999999995, 0.09740000000000001],\n",
       " [0.11408333333333333, 0.03841666666666666, 0.08116666666666668],\n",
       " [0.04050000000000001, 0.020555555555555556, 0.027777777777777776],\n",
       " [0.14085714285714288, 0.08219047619047619, 0.12066666666666666],\n",
       " [0.19325, 0.14510000000000003, 0.15839999999999999],\n",
       " [0.17466666666666666, 0.15433333333333335, 0.15325],\n",
       " [0.2152857142857143, 0.09814285714285716, 0.14928571428571427],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.04957142857142858, 0.038642857142857145, 0.05121428571428572],\n",
       " [0.23466666666666666, 0.16973333333333335, 0.20126666666666668],\n",
       " [0.18215625, 0.11921874999999998, 0.1505625],\n",
       " [0.16109999999999997, 0.22609999999999997, 0.15575],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.066375, 0.026875, 0.04625],\n",
       " [0.0999142857142857, 0.05474285714285715, 0.06914285714285714],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.13704761904761906, 0.07376190476190476, 0.09723809523809523],\n",
       " [0.17476666666666668, 0.09200000000000003, 0.14363333333333336],\n",
       " [0.23592682926829273, 0.13480487804878047, 0.18592682926829265],\n",
       " [0.13364705882352942, 0.08658823529411765, 0.10529411764705883],\n",
       " [0.20365714285714287, 0.1248857142857143, 0.1326857142857143],\n",
       " [0.24383333333333332, 0.11808333333333333, 0.1668333333333333],\n",
       " [0.2567272727272727, 0.14481818181818182, 0.1801818181818182],\n",
       " [0.24358823529411766, 0.12494117647058824, 0.1963529411764706],\n",
       " [0.10663636363636364, 0.04818181818181819, 0.0829090909090909],\n",
       " [0.1888, 0.143, 0.1766],\n",
       " [0.17036, 0.15344, 0.17484000000000002],\n",
       " [0.17118181818181818, 0.11436363636363636, 0.13154545454545455],\n",
       " [0.151, 0.08707142857142856, 0.10285714285714286],\n",
       " [0.2656666666666666, 0.21908333333333332, 0.211],\n",
       " [0.25503999999999993, 0.21032, 0.20256],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.06659090909090909, 0.08063636363636363, 0.06409090909090909],\n",
       " [0.1267894736842105, 0.11707894736842105, 0.11605263157894737],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.12868000000000002, 0.09904, 0.12288],\n",
       " [0.13958333333333336, 0.084, 0.11491666666666667],\n",
       " [0.07875, 0.05471428571428571, 0.07735714285714286],\n",
       " [0.21875, 0.13665000000000002, 0.15805],\n",
       " [0.1865, 0.12373333333333332, 0.13623333333333332],\n",
       " [0.10017857142857142, 0.07342857142857143, 0.08882142857142858],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.061222222222222226, 0.03322222222222222, 0.06966666666666667],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.15009090909090908, 0.11127272727272729, 0.09963636363636365],\n",
       " [0.12284210526315788, 0.08957894736842105, 0.09336842105263159],\n",
       " [0.10391666666666666, 0.05566666666666666, 0.08600000000000001],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.21373333333333336, 0.10766666666666665, 0.11046666666666666],\n",
       " [0.13846666666666668, 0.07526666666666666, 0.07266666666666666],\n",
       " [0.04989473684210526, 0.033842105263157896, 0.027526315789473687],\n",
       " [0.19688000000000003, 0.09295999999999999, 0.13116000000000003],\n",
       " [0.11505263157894736, 0.04278947368421053, 0.094],\n",
       " [0.18066666666666667, 0.09579166666666666, 0.15258333333333332],\n",
       " [0.23888888888888893, 0.1651111111111111, 0.20844444444444443],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.06366666666666666, 0.03333333333333333, 0.04611111111111111],\n",
       " [0.2476666666666667, 0.15983333333333333, 0.17925000000000002],\n",
       " [0.08627272727272728, 0.040818181818181816, 0.057],\n",
       " [0.11130434782608696, 0.08582608695652173, 0.09999999999999999],\n",
       " [0.08, 0.10458333333333332, 0.08841666666666666],\n",
       " [0.10852631578947369, 0.10857894736842107, 0.10221052631578947],\n",
       " [0.22494444444444447, 0.15033333333333332, 0.1756666666666667],\n",
       " [0.19476470588235295, 0.12811764705882353, 0.17770588235294116],\n",
       " [0.2335, 0.14222222222222222, 0.19638888888888892],\n",
       " [0.17871428571428566, 0.12114285714285712, 0.14310714285714285],\n",
       " [0.15025, 0.09337500000000001, 0.087875],\n",
       " [0.23929999999999998, 0.14684999999999998, 0.19005],\n",
       " [0.16864, 0.10099999999999999, 0.11852],\n",
       " [0.22228571428571428, 0.1137142857142857, 0.18085714285714286],\n",
       " [0.1996, 0.14412000000000003, 0.16879999999999998],\n",
       " [0.14425641025641026, 0.12851282051282054, 0.14287179487179488],\n",
       " [0.17007999999999998, 0.10976, 0.1638],\n",
       " [0.2311764705882353, 0.12976470588235295, 0.20011764705882354],\n",
       " [0.23457142857142857, 0.12514285714285714, 0.19285714285714287],\n",
       " [0.12281249999999999, 0.0689375, 0.1063125],\n",
       " [0.055839999999999994, 0.05932, 0.06444],\n",
       " [0.12351219512195122, 0.09787804878048781, 0.1128048780487805],\n",
       " [0.1620714285714286, 0.11253571428571428, 0.14214285714285713],\n",
       " [0.22432258064516128, 0.1464516129032258, 0.18467741935483872],\n",
       " [0.099, 0.0720625, 0.0904375],\n",
       " [0.08800000000000001, 0.06405555555555556, 0.0803888888888889],\n",
       " [0.11314285714285714, 0.08235714285714286, 0.10335714285714286],\n",
       " [0.046642857142857146, 0.0315, 0.054214285714285715],\n",
       " [0.05023076923076923, 0.033923076923076924, 0.05838461538461538],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.216, 0.12733333333333333, 0.17020833333333338],\n",
       " [0.1389130434782609, 0.07695652173913044, 0.1082608695652174],\n",
       " [0.14403030303030306, 0.08481818181818182, 0.12309090909090907],\n",
       " [0.15526315789473685, 0.09557894736842104, 0.14073684210526316],\n",
       " [0.16557142857142856, 0.11714285714285715, 0.1522857142857143],\n",
       " [0.178, 0.10506666666666667, 0.12953333333333333],\n",
       " [0.19236111111111112, 0.11755555555555554, 0.16386111111111112],\n",
       " [0.17618421052631583, 0.11721052631578947, 0.15297368421052632],\n",
       " [0.14055555555555557, 0.10228888888888889, 0.1258888888888889],\n",
       " [0.17863636363636362, 0.13022727272727275, 0.17672727272727273],\n",
       " [0.11942105263157896, 0.09063157894736842, 0.09163157894736841],\n",
       " [0.13378947368421054, 0.07431578947368421, 0.09184210526315789],\n",
       " [0.15660714285714286, 0.12303571428571429, 0.12585714285714286],\n",
       " [0.16072727272727272, 0.13254545454545452, 0.13622727272727275],\n",
       " [0.1778125, 0.12103125, 0.1390625],\n",
       " [0.15665517241379312, 0.12917241379310343, 0.1353103448275862],\n",
       " [0.1073125, 0.0675625, 0.0868125],\n",
       " [0.18414285714285714, 0.11380952380952382, 0.14980952380952384],\n",
       " [0.09177777777777779, 0.04533333333333334, 0.032888888888888884],\n",
       " [0.1836470588235294, 0.16205882352941176, 0.13441176470588237],\n",
       " [0.10500000000000001, 0.04206666666666667, 0.085],\n",
       " [0.14404761904761906, 0.09228571428571428, 0.10138095238095238],\n",
       " [0.11126315789473684, 0.07247368421052632, 0.07268421052631578],\n",
       " [0.2251, 0.1444, 0.1863],\n",
       " [0.22184848484848488, 0.13124242424242422, 0.18660606060606058],\n",
       " [0.08578571428571428, 0.06799999999999999, 0.07957142857142856],\n",
       " [0.1375185185185185, 0.11111111111111113, 0.13422222222222221],\n",
       " [0.06833333333333333, 0.03516666666666667, 0.0585],\n",
       " [0.2465263157894737, 0.16421052631578945, 0.19789473684210526],\n",
       " [0.2188, 0.14112, 0.17512],\n",
       " [0.15320833333333334, 0.08950000000000001, 0.11858333333333333],\n",
       " [0.1947, 0.144, 0.145],\n",
       " [0.10892307692307691, 0.06807692307692308, 0.06892307692307692],\n",
       " [0.2262307692307692, 0.14076923076923079, 0.16407692307692306],\n",
       " [0.0951, 0.11779999999999999, 0.10783333333333335],\n",
       " [0.025954545454545456, 0.014090909090909091, 0.02190909090909091],\n",
       " [0.15122727272727274, 0.08574999999999999, 0.11204545454545455],\n",
       " [0.12982758620689655, 0.10017241379310343, 0.10593103448275862],\n",
       " [0.15840909090909092, 0.10886363636363634, 0.11736363636363635],\n",
       " [0.16504761904761905, 0.1250952380952381, 0.13028571428571428],\n",
       " [0.13864, 0.10507999999999999, 0.10944],\n",
       " [0.13790322580645162, 0.07819354838709676, 0.08170967741935484],\n",
       " [0.21352941176470588, 0.12123529411764705, 0.1478823529411765],\n",
       " [0.10061538461538462, 0.056846153846153845, 0.08076923076923077],\n",
       " [0.224, 0.10746153846153847, 0.14707692307692308],\n",
       " [0.1033793103448276, 0.08344827586206896, 0.08324137931034482],\n",
       " [0.11419354838709678, 0.0877741935483871, 0.08741935483870968],\n",
       " [0.2118936170212766, 0.11612765957446809, 0.16093617021276593],\n",
       " [0.1585925925925926, 0.08122222222222222, 0.1385185185185185],\n",
       " [0.199625, 0.11171874999999999, 0.17396875],\n",
       " [0.2822857142857143, 0.15642857142857142, 0.2162142857142857],\n",
       " [0.08005263157894736, 0.0628421052631579, 0.07757894736842105],\n",
       " [0.1443333333333333, 0.07388888888888889, 0.10866666666666666],\n",
       " [0.23615384615384613, 0.12084615384615384, 0.16776923076923078],\n",
       " [0.15499999999999997, 0.099, 0.0858],\n",
       " [0.1244, 0.0734, 0.0526],\n",
       " [0.185, 0.08800000000000001, 0.1775],\n",
       " [0.11144444444444446, 0.12200000000000001, 0.11866666666666667],\n",
       " [0.05490909090909091, 0.023545454545454546, 0.024],\n",
       " [0.08713333333333333, 0.03773333333333333, 0.05493333333333334],\n",
       " [0.14996551724137933, 0.16137931034482758, 0.153551724137931],\n",
       " [0.06881818181818182, 0.03536363636363637, 0.053909090909090907],\n",
       " [0.08411111111111111, 0.043222222222222224, 0.06588888888888889],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.085, 0.0735, 0.074],\n",
       " [0.07285714285714286, 0.063, 0.06342857142857143],\n",
       " [0.13425, 0.150625, 0.14604166666666665],\n",
       " [0.09172222222222222, 0.05616666666666666, 0.07888888888888888],\n",
       " [0.1400357142857143, 0.13732142857142857, 0.13264285714285715],\n",
       " [0.19637500000000002, 0.103625, 0.144375],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.26873913043478265, 0.12052173913043478, 0.18160869565217388],\n",
       " [0.19241176470588237, 0.07158823529411765, 0.1255294117647059],\n",
       " [0.17879166666666668, 0.13366666666666668, 0.15770833333333334],\n",
       " [0.16585714285714287, 0.08064285714285714, 0.1262142857142857],\n",
       " [0.03934090909090909, 0.03206818181818182, 0.039636363636363636],\n",
       " [0.06961538461538462, 0.052307692307692305, 0.07430769230769231],\n",
       " [0.04644444444444444, 0.036333333333333336, 0.04488888888888889],\n",
       " [0.05123076923076923, 0.07946153846153846, 0.05838461538461538],\n",
       " [0.05435714285714285, 0.08121428571428571, 0.07171428571428572],\n",
       " [0.09393750000000001, 0.053156249999999995, 0.086625],\n",
       " [0.06090909090909091, 0.04981818181818181, 0.04872727272727273],\n",
       " [0.1311, 0.1325, 0.10969999999999999],\n",
       " [0.060149999999999995, 0.10535000000000001, 0.0629],\n",
       " [0.13082352941176473, 0.1371764705882353, 0.113],\n",
       " [0.13152380952380952, 0.08547619047619047, 0.1227142857142857],\n",
       " [0.08363636363636363, 0.048, 0.052636363636363634],\n",
       " [0.10462500000000001, 0.0745, 0.10612500000000001],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.09371428571428571, 0.04857142857142858, 0.11],\n",
       " [0.11457142857142856, 0.08885714285714286, 0.1447857142857143],\n",
       " [0.079, 0.07533333333333332, 0.10475000000000001],\n",
       " [0.15646153846153849, 0.08523076923076922, 0.111],\n",
       " [0.19504545454545455, 0.11063636363636363, 0.14581818181818182],\n",
       " [0.1602857142857143, 0.11142857142857142, 0.11917857142857144],\n",
       " [0.17465384615384613, 0.09684615384615386, 0.13534615384615384],\n",
       " [0.19435714285714287, 0.10564285714285715, 0.1465],\n",
       " [0.1533846153846154, 0.07576923076923078, 0.10076923076923078],\n",
       " [0.0911875, 0.094125, 0.081],\n",
       " [0.07343749999999999, 0.06175, 0.08956249999999999],\n",
       " [0.13366666666666666, 0.11533333333333334, 0.14041666666666666],\n",
       " [0.11231999999999999, 0.062400000000000004, 0.0828],\n",
       " [0.15333333333333332, 0.13122222222222224, 0.10955555555555556],\n",
       " [0.08375, 0.06849999999999999, 0.067],\n",
       " [0.13694736842105265, 0.079, 0.11142105263157895],\n",
       " [0.08484375, 0.13971875, 0.12315625000000001],\n",
       " [0.11336363636363635, 0.09777272727272726, 0.10881818181818183],\n",
       " [0.3136923076923077, 0.2062307692307692, 0.247],\n",
       " [0.18044444444444446, 0.17, 0.07955555555555555],\n",
       " [0.13855555555555554, 0.09766666666666667, 0.10955555555555556],\n",
       " [0.18863999999999997, 0.13943999999999998, 0.1692],\n",
       " [0.17995454545454545, 0.1407727272727273, 0.1653181818181818],\n",
       " [0.09793548387096775, 0.05729032258064516, 0.07096774193548387],\n",
       " [0.09341304347826086, 0.05758695652173913, 0.06695652173913044],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.12040000000000002, 0.07980000000000001, 0.0751],\n",
       " [0.1790625, 0.09581250000000001, 0.14600000000000002],\n",
       " [0.135125, 0.07770833333333334, 0.10779166666666666],\n",
       " [0.21960869565217392, 0.2912608695652174, 0.19869565217391305],\n",
       " [0.1615, 0.1095, 0.10899999999999999],\n",
       " [0.19494999999999998, 0.1644, 0.13529999999999998],\n",
       " [0.23940909090909093, 0.1584545454545455, 0.21336363636363637],\n",
       " [0.10762499999999998, 0.07687499999999999, 0.10570833333333333],\n",
       " [0.1096086956521739, 0.05221739130434783, 0.09226086956521738],\n",
       " [0.05823076923076923, 0.029923076923076924, 0.04561538461538461],\n",
       " [0.10816, 0.07456, 0.10148],\n",
       " [0.15333333333333332, 0.10618518518518519, 0.1421851851851852],\n",
       " [0.21564999999999998, 0.16720000000000002, 0.19285000000000002],\n",
       " [0.14742424242424243, 0.11348484848484851, 0.12875757575757574],\n",
       " [0.08880645161290322, 0.05693548387096774, 0.07735483870967742],\n",
       " [0.1018235294117647, 0.05758823529411764, 0.09629411764705882],\n",
       " [0.19924, 0.10944, 0.16804000000000002],\n",
       " [0.14611428571428572, 0.10457142857142858, 0.11437142857142857],\n",
       " [0.18183333333333332, 0.11161111111111113, 0.13405555555555557],\n",
       " [0.1335, 0.12166666666666666, 0.13422222222222221],\n",
       " [0.20755555555555555, 0.15174074074074073, 0.18407407407407408],\n",
       " [0.19605882352941176, 0.1342941176470588, 0.18399999999999997],\n",
       " [0.15842857142857142, 0.0907142857142857, 0.13842857142857143],\n",
       " [0.2123181818181818, 0.1633181818181818, 0.1741818181818182],\n",
       " [0.21528124999999998, 0.1731875, 0.1730625],\n",
       " [0.14786666666666667, 0.12993333333333335, 0.11373333333333333],\n",
       " [0.11407692307692309, 0.07061538461538462, 0.10369230769230768],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.2297142857142857, 0.1459047619047619, 0.17442857142857143],\n",
       " [0.2297142857142857, 0.1459047619047619, 0.17442857142857143],\n",
       " [0.09825, 0.1004375, 0.084625],\n",
       " [0.15125000000000002, 0.1193, 0.1318],\n",
       " [0.149, 0.12353846153846153, 0.14984615384615385],\n",
       " [0.04352, 0.05512, 0.04888],\n",
       " [0.04114285714285714, 0.05766666666666667, 0.04561904761904762],\n",
       " [0.14963636363636362, 0.1392727272727273, 0.143],\n",
       " [0.16495238095238096, 0.10133333333333334, 0.13595238095238096],\n",
       " [0.17274358974358972, 0.1078717948717949, 0.1545897435897436],\n",
       " [0.13529032258064516, 0.10029032258064517, 0.1355483870967742],\n",
       " [0.17516666666666666, 0.11749999999999998, 0.15466666666666667],\n",
       " [0.10066666666666667, 0.065, 0.07916666666666666],\n",
       " [0.14004545454545456, 0.08313636363636365, 0.1065],\n",
       " [0.15288000000000002, 0.09356, 0.11428],\n",
       " [0.19260000000000002, 0.1488, 0.18570000000000003],\n",
       " [0.24682352941176475, 0.15464705882352942, 0.17441176470588232],\n",
       " [0.23311111111111116, 0.14605555555555555, 0.1647222222222222],\n",
       " [0.12054545454545455, 0.0720909090909091, 0.06563636363636363],\n",
       " [0.1408, 0.08006666666666667, 0.08933333333333332],\n",
       " [0.14406666666666668, 0.07553333333333333, 0.08373333333333334],\n",
       " [0.1812258064516129, 0.09251612903225807, 0.13041935483870967],\n",
       " [0.09690000000000001, 0.0844, 0.057499999999999996],\n",
       " [0.14041176470588235, 0.0908235294117647, 0.08294117647058824],\n",
       " [0.18246153846153845, 0.146, 0.1614615384615385],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.18163636363636365, 0.10127272727272726, 0.13436363636363638],\n",
       " [0.1665, 0.09283333333333332, 0.12316666666666669],\n",
       " [0.19885714285714287, 0.11995238095238094, 0.16228571428571428],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.21762499999999999, 0.14965625, 0.16325],\n",
       " [0.16605714285714288, 0.14202857142857142, 0.16002857142857146],\n",
       " [0.22411764705882356, 0.14423529411764704, 0.18300000000000005],\n",
       " [0.21263636363636368, 0.16072727272727275, 0.18754545454545457],\n",
       " [0.18438235294117647, 0.1646176470588235, 0.1671176470588235],\n",
       " [0.14650000000000002, 0.083125, 0.109375],\n",
       " [0.060399999999999995, 0.023100000000000002, 0.04],\n",
       " [0.19878947368421054, 0.14228947368421052, 0.16907894736842102],\n",
       " [0.04726315789473684, 0.02336842105263158, 0.02736842105263158],\n",
       " [0.1466, 0.10088, 0.11544],\n",
       " [0.17866666666666667, 0.12777777777777777, 0.14588888888888887],\n",
       " [0.161, 0.10366666666666667, 0.13855555555555554],\n",
       " [0.17648, 0.10640000000000001, 0.15224],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.052, 0.06366666666666666, 0.034833333333333334],\n",
       " [0.07446666666666667, 0.11446666666666667, 0.09406666666666667],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.19725, 0.10868749999999999, 0.16125],\n",
       " [0.11975, 0.083125, 0.10125],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.15845454545454546, 0.07754545454545454, 0.09586363636363636],\n",
       " [0.07116666666666667, 0.06216666666666667, 0.051166666666666666],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.1572, 0.0861, 0.10900000000000001],\n",
       " [0.19422222222222224, 0.10972222222222222, 0.15000000000000002],\n",
       " [0.17361538461538462, 0.09853846153846155, 0.14046153846153844],\n",
       " [0.06659999999999999, 0.0334, 0.0432],\n",
       " [0.2630357142857143, 0.1629642857142857, 0.21610714285714286],\n",
       " [0.0, 0.0, 0.0],\n",
       " [0.08990909090909091, 0.10436363636363635, 0.09672727272727273],\n",
       " [0.1502857142857143, 0.12742857142857142, 0.0982857142857143],\n",
       " [0.20749999999999996, 0.20575, 0.16975],\n",
       " [0.2020769230769231, 0.1583846153846154, 0.1995],\n",
       " [0.13890000000000002, 0.09009999999999999, 0.0982],\n",
       " [0.11008333333333335, 0.12724999999999997, 0.12666666666666668],\n",
       " [0.21773913043478266, 0.13365217391304346, 0.18969565217391307],\n",
       " [0.21869444444444444, 0.1446944444444444, 0.15713888888888888],\n",
       " [0.12734883720930235, 0.08388372093023257, 0.11411627906976746],\n",
       " [0.29647999999999997, 0.15800000000000003, 0.23876],\n",
       " [0.16394444444444448, 0.08333333333333333, 0.13144444444444442],\n",
       " [0.2383809523809524, 0.13447619047619047, 0.1884761904761905],\n",
       " [0.13022222222222224, 0.060000000000000005, 0.08777777777777779],\n",
       " [0.05411111111111112, 0.03922222222222222, 0.06244444444444445],\n",
       " [0.0907241379310345, 0.055413793103448274, 0.06517241379310346],\n",
       " [0.043, 0.023, 0.025],\n",
       " [0.0978181818181818, 0.056545454545454545, 0.06318181818181819],\n",
       " [0.121, 0.08193333333333334, 0.115],\n",
       " [0.093, 0.07005, 0.09605],\n",
       " [0.09559999999999999, 0.08486666666666667, 0.08546666666666666],\n",
       " [0.178, 0.10763636363636366, 0.13481818181818184],\n",
       " [0.2293529411764706, 0.16541176470588237, 0.1678235294117647],\n",
       " [0.16666666666666666, 0.12626666666666667, 0.12113333333333333],\n",
       " [0.085, 0.04011111111111111, 0.07822222222222223],\n",
       " [0.1539090909090909, 0.08904545454545455, 0.1040909090909091],\n",
       " [0.1738235294117647, 0.07764705882352942, 0.1373529411764706],\n",
       " [0.1846875, 0.08249999999999999, 0.1459375],\n",
       " [0.25, 0.12975, 0.16824999999999998],\n",
       " [0.14285714285714285, 0.07414285714285715, 0.09614285714285713],\n",
       " [0.22275, 0.13125, 0.14400000000000002],\n",
       " [0.21621052631578946, 0.1913157894736842, 0.2001578947368421],\n",
       " [0.17900000000000002, 0.12296296296296295, 0.17274074074074072],\n",
       " [0.07253571428571429, 0.05225, 0.046892857142857146],\n",
       " [0.06344444444444446, 0.034444444444444444, 0.05355555555555556],\n",
       " [0.20548148148148146, 0.12218518518518516, 0.16103703703703703],\n",
       " [0.1697142857142857, 0.09938095238095238, 0.13223809523809524],\n",
       " [0.09267857142857143, 0.07242857142857143, 0.08089285714285714],\n",
       " [0.05783333333333334, 0.045083333333333336, 0.059750000000000004],\n",
       " [0.18116666666666667, 0.1401111111111111, 0.14733333333333332],\n",
       " [0.095125, 0.09475, 0.101625],\n",
       " [0.095125, 0.09475, 0.101625],\n",
       " [0.21551851851851853, 0.12403703703703703, 0.16885185185185186],\n",
       " [0.1898571428571428, 0.10660714285714286, 0.13832142857142857],\n",
       " [0.10800000000000001, 0.08093333333333333, 0.08826666666666666],\n",
       " [0.1111111111111111, 0.05766666666666667, 0.07477777777777778],\n",
       " [0.16666666666666666, 0.08650000000000001, 0.11216666666666665],\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/zhiyuan/ENTER/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sentence before tokenization:  My duties?  All right. [SEP] Now you ll be heading a whole division, so you ll have a lot of duties.\n",
      "Encoded Input from dataset:  [101, 2026, 5704, 1029, 2035, 2157, 1012, 102, 2085, 2017, 2222, 2022, 5825, 1037, 2878, 2407, 1010, 2061, 2017, 2222, 2031, 1037, 2843, 1997, 5704, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "\n",
    "MAX_LEN = 256\n",
    "\n",
    "## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\n",
    "\n",
    "print(\"Actual sentence before tokenization: \",sentences[2])\n",
    "print(\"Encoded Input from dataset: \",input_ids[2])\n",
    "\n",
    "## Create attention mask\n",
    "attention_masks = []\n",
    "## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "print(attention_masks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
    "\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "\n",
    "train_inputs,test_inputs,train_labels,test_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\n",
    "train_masks,test_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)\n",
    "train_init_emos,test_init_emos,_,_ = train_test_split(init_emo,input_ids,random_state=41,test_size=0.1)\n",
    "\n",
    "train_set_labels = train_labels\n",
    "\n",
    "\n",
    "train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(train_inputs,train_set_labels,random_state=41,test_size=0.1)\n",
    "train_masks,validation_masks,_,_ = train_test_split(train_masks,train_set_labels,random_state=41,test_size=0.1)\n",
    "train_init_emos,validation_init_emos,_,_ = train_test_split(train_init_emos,train_set_labels,random_state=41,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# convert all our data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "train_init_emos = torch.tensor(train_init_emos)\n",
    "validation_init_emos = torch.tensor(validation_init_emos)\n",
    "test_init_emos = torch.tensor(test_init_emos)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_init_emos,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_init_emos,validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs,test_masks,test_init_emos,test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 10166,  1010,  2008,  2003,  1037,  4474,  1012,  2074,  2028,\n",
       "          2210,  3160,  1010,  7910,  1010,  2339,  2025,  5811,  1005,  1055,\n",
       "          2282,  1029,   102,  2175,  4095,  1010,  2057,  5720,  2055,  2008,\n",
       "          2021,  2115,  2567,  2038,  2061,  2116,  2671, 22236,  1998, 28487,\n",
       "          1998,  7857, 23433,  1010,  2092,  2057,  2134,  1005,  1056,  2215,\n",
       "          2000, 22995,  2068,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " tensor([0.1538, 0.1178, 0.1297]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Head for sentence-level classification tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()        \n",
    "        self.dense = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class Emo_Generation(BertPreTrainedModel):\n",
    "    def __init__(self, config, mode):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 7\n",
    "        self.mid_size = 100 \n",
    "        self.bert = BertModel(config)\n",
    "        self.mode = mode\n",
    "\n",
    "        # self.reduce = nn.Linear(config.hidden_size*2, config.hidden_size)\n",
    "        \n",
    "        \n",
    "        if mode == 1: # mode 1: directlly classify with bert embedding;\n",
    "            self.utter_classifier = nn.Linear(config.hidden_size, 7)\n",
    "        elif mode == 2: # mode 2: concat bert embedding and personality;\n",
    "            self.personality_trans = nn.Linear(5, self.mid_size) # 5-d personality vec     \n",
    "        elif mode == 3: # mode 3: personality-based emotion transition;\n",
    "            self.utter_classifier = nn.Linear(config.hidden_size, 3)\n",
    "            # self.vad_para_trans = nn.Linear(3, 3) \n",
    "            # self.vad_to_hidden = nn.Linear(3, config.hidden_size)\n",
    "            \n",
    "            self.init_transfer = ClassificationHead(3, config.hidden_size, 3)\n",
    "            self.cls_head = ClassificationHead(3, config.hidden_size, 7)\n",
    "            \n",
    "\n",
    "    def personality_to_vad(self, personality):\n",
    "        O, C, E, A, N = personality[:, 0], personality[:, 1], personality[:, 2], personality[:, 3], personality[:, 4]\n",
    "        \n",
    "        valence = 0.21 * E + 0.59 * A + 0.19 * N\n",
    "        arousal = 0.15 * O + 0.30 * A - 0.57 * N\n",
    "        dominance = 0.25 * O + 0.17 * C + 0.60 * E - 0.32 * A\n",
    "\n",
    "        # valence = E + A + N\n",
    "        # arousal = O + A - N\n",
    "        # dominance = O + C + E - A\n",
    "\n",
    "        return torch.cat((valence.unsqueeze(-1), arousal.unsqueeze(-1), dominance.unsqueeze(-1)), 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, init_emo=None):\n",
    "        bert_outputs = self.bert(input_ids, attention_mask)\n",
    "        bert_hidden = bert_outputs[1]\n",
    "        \n",
    "        if self.mode == 1:\n",
    "            logits = self.utter_classifier(bert_hidden)\n",
    "        elif self.mode == 2: \n",
    "            personality = self.personality_trans(personality.cuda(device))\n",
    "            logits = self.utter_classifier(bert_hidden, personality)        \n",
    "        elif self.mode == 3:\n",
    "            utter_emo = self.utter_classifier(bert_hidden) # delta of v, a, d\n",
    "            init_emo = self.init_transfer(init_emo)\n",
    "            target_emo = init_emo + utter_emo# * personality_influence\n",
    "            logits = self.cls_head(utter_emo)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing Emo_Generation: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing Emo_Generation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Emo_Generation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Emo_Generation were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['init_transfer.out_proj.bias', 'init_transfer.dense.weight', 'cls_head.dense.weight', 'utter_classifier.bias', 'cls_head.dense.bias', 'cls_head.out_proj.bias', 'utter_classifier.weight', 'init_transfer.dense.bias', 'init_transfer.out_proj.weight', 'cls_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=7).cuda(1)\n",
    "\n",
    "\n",
    "model = Emo_Generation.from_pretrained('bert-base-uncased', mode=3).cuda(1)\n",
    "# model = Emo_Generation(mode=3).cuda(1)\n",
    "\n",
    "\n",
    "\n",
    "# Parameters:\n",
    "lr = 1e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 50\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#         if name.startswith('bert'):\n",
    "#             param.requires_grad = False\n",
    "#         else:\n",
    "#             pass\n",
    "#         if name.startswith('bert.encoder.layer.11') or name.startswith('bert.pooler'):\n",
    "#             param.requires_grad = True\n",
    "              \n",
    "\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b7cd94b85907>:14: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1,epochs+1,desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cda0d5084a146bda68933142ab2b406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  9.800000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 1.8749165225560498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyuan/ENTER/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0000    0.0000    0.0000         0\n",
      "         1.0     0.0000    0.0000    0.0000         0\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     1.0000    0.4297    0.6011       619\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4297       619\n",
      "   macro avg     0.1429    0.0614    0.0859       619\n",
      "weighted avg     1.0000    0.4297    0.6011       619\n",
      "\n",
      "<====================== Epoch 2 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  9.600000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 1.8663953565488196\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0000    0.0000    0.0000         0\n",
      "         1.0     0.0000    0.0000    0.0000         0\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     1.0000    0.4297    0.6011       619\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.4297       619\n",
      "   macro avg     0.1429    0.0614    0.0859       619\n",
      "weighted avg     1.0000    0.4297    0.6011       619\n",
      "\n",
      "<====================== Epoch 3 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  9.4e-06\n",
      "\n",
      "\tAverage Training loss: 1.8475467065337356\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0779    0.2222    0.1154        27\n",
      "         1.0     0.1250    0.0286    0.0465        70\n",
      "         2.0     0.0244    0.1000    0.0392        10\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.8797    0.4570    0.6015       512\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3926       619\n",
      "   macro avg     0.1581    0.1154    0.1147       619\n",
      "weighted avg     0.7456    0.3926    0.5085       619\n",
      "\n",
      "<====================== Epoch 4 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  9.200000000000002e-06\n",
      "\n",
      "\tAverage Training loss: 1.7617483108666292\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0130    0.3333    0.0250         3\n",
      "         1.0     0.1875    0.0370    0.0619        81\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.9098    0.4523    0.6042       535\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3974       619\n",
      "   macro avg     0.1586    0.1175    0.0987       619\n",
      "weighted avg     0.8109    0.3974    0.5305       619\n",
      "\n",
      "<====================== Epoch 5 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  9e-06\n",
      "\n",
      "\tAverage Training loss: 1.6554717896090951\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0260    0.1429    0.0440        14\n",
      "         1.0     0.1875    0.0181    0.0330       166\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.7519    0.4556    0.5674       439\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3312       619\n",
      "   macro avg     0.1379    0.0881    0.0920       619\n",
      "weighted avg     0.5841    0.3312    0.4122       619\n",
      "\n",
      "<====================== Epoch 6 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  8.8e-06\n",
      "\n",
      "\tAverage Training loss: 1.5625895269357475\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0260    0.1053    0.0417        19\n",
      "         1.0     0.1250    0.0294    0.0476        68\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.8947    0.4474    0.5965       532\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3910       619\n",
      "   macro avg     0.1494    0.0831    0.0980       619\n",
      "weighted avg     0.7835    0.3910    0.5192       619\n",
      "\n",
      "<====================== Epoch 7 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  8.6e-06\n",
      "\n",
      "\tAverage Training loss: 1.4818493389779595\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.1039    0.1379    0.1185        58\n",
      "         1.0     0.1875    0.0189    0.0343       159\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.6805    0.4502    0.5419       402\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3102       619\n",
      "   macro avg     0.1388    0.0867    0.0992       619\n",
      "weighted avg     0.4998    0.3102    0.3719       619\n",
      "\n",
      "<====================== Epoch 8 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  8.400000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 1.4043202721009589\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0779    0.1132    0.0923        53\n",
      "         1.0     0.0625    0.0222    0.0328        45\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.8647    0.4415    0.5845       521\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3829       619\n",
      "   macro avg     0.1436    0.0824    0.1014       619\n",
      "weighted avg     0.7390    0.3829    0.5022       619\n",
      "\n",
      "<====================== Epoch 9 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  8.2e-06\n",
      "\n",
      "\tAverage Training loss: 1.3537055519735737\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.3117    0.1579    0.2096       152\n",
      "         1.0     0.1250    0.0299    0.0482        67\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.6992    0.4650    0.5586       400\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3425       619\n",
      "   macro avg     0.1623    0.0932    0.1166       619\n",
      "weighted avg     0.5419    0.3425    0.4176       619\n",
      "\n",
      "<====================== Epoch 10 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  8.000000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 1.2678008741064437\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4545    0.1667    0.2439       210\n",
      "         1.0     0.0625    0.0250    0.0357        40\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.6654    0.4797    0.5575       369\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3441       619\n",
      "   macro avg     0.1689    0.0959    0.1196       619\n",
      "weighted avg     0.5549    0.3441    0.4174       619\n",
      "\n",
      "<====================== Epoch 11 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  7.800000000000002e-06\n",
      "\n",
      "\tAverage Training loss: 1.2136364571607796\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4935    0.1597    0.2413       238\n",
      "         1.0     0.0625    0.0909    0.0741        11\n",
      "         2.0     0.0000    0.0000    0.0000         2\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.6504    0.4701    0.5457       368\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3425       619\n",
      "   macro avg     0.1723    0.1030    0.1230       619\n",
      "weighted avg     0.5775    0.3425    0.4185       619\n",
      "\n",
      "<====================== Epoch 12 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  7.600000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 1.1495671122317102\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5325    0.1602    0.2462       256\n",
      "         1.0     0.0625    0.0714    0.0667        14\n",
      "         2.0     0.0244    0.2500    0.0444         4\n",
      "         3.0     0.0000    0.0000    0.0000         0\n",
      "         4.0     0.6316    0.4870    0.5499       345\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3409       619\n",
      "   macro avg     0.1787    0.1384    0.1296       619\n",
      "weighted avg     0.5738    0.3409    0.4101       619\n",
      "\n",
      "<====================== Epoch 13 ======================>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCurrent Learning rate:  7.4e-06\n",
      "\n",
      "\tAverage Training loss: 1.1024649481105198\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4675    0.1417    0.2175       254\n",
      "         1.0     0.0625    0.0909    0.0741        11\n",
      "         2.0     0.0244    0.1111    0.0400         9\n",
      "         3.0     0.0000    0.0000    0.0000         7\n",
      "         4.0     0.6090    0.4793    0.5364       338\n",
      "         5.0     0.0000    0.0000    0.0000         0\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3231       619\n",
      "   macro avg     0.1662    0.1176    0.1240       619\n",
      "weighted avg     0.5259    0.3231    0.3841       619\n",
      "\n",
      "<====================== Epoch 14 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  7.2000000000000005e-06\n",
      "\n",
      "\tAverage Training loss: 1.0626954407828628\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4156    0.1410    0.2105       227\n",
      "         1.0     0.0000    0.0000    0.0000        10\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0521    0.2083    0.0833        24\n",
      "         4.0     0.6128    0.4766    0.5362       342\n",
      "         5.0     0.0192    0.0625    0.0294        16\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3247       619\n",
      "   macro avg     0.1571    0.1269    0.1228       619\n",
      "weighted avg     0.4935    0.3247    0.3774       619\n",
      "\n",
      "<====================== Epoch 15 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  7e-06\n",
      "\n",
      "\tAverage Training loss: 1.0179983128787606\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4935    0.1751    0.2585       217\n",
      "         1.0     0.0000    0.0000    0.0000         8\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.1354    0.2500    0.1757        52\n",
      "         4.0     0.5902    0.4801    0.5295       327\n",
      "         5.0     0.0192    0.0667    0.0299        15\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3376       619\n",
      "   macro avg     0.1769    0.1388    0.1419       619\n",
      "weighted avg     0.4966    0.3376    0.3858       619\n",
      "\n",
      "<====================== Epoch 16 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  6.800000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 0.9882487968844214\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4805    0.1897    0.2721       195\n",
      "         1.0     0.0000    0.0000    0.0000         6\n",
      "         2.0     0.0244    0.1429    0.0417         7\n",
      "         3.0     0.1771    0.2833    0.2179        60\n",
      "         4.0     0.5902    0.4906    0.5358       320\n",
      "         5.0     0.0962    0.1613    0.1205        31\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3506       619\n",
      "   macro avg     0.1955    0.1811    0.1697       619\n",
      "weighted avg     0.4788    0.3506    0.3903       619\n",
      "\n",
      "<====================== Epoch 17 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  6.600000000000001e-06\n",
      "\n",
      "\tAverage Training loss: 0.9275197937230396\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4545    0.1852    0.2632       189\n",
      "         1.0     0.0000    0.0000    0.0000         6\n",
      "         2.0     0.0000    0.0000    0.0000         4\n",
      "         3.0     0.1979    0.1743    0.1854       109\n",
      "         4.0     0.5075    0.4945    0.5009       273\n",
      "         5.0     0.1346    0.1842    0.1556        38\n",
      "         6.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.3166       619\n",
      "   macro avg     0.1849    0.1483    0.1579       619\n",
      "weighted avg     0.4057    0.3166    0.3435       619\n",
      "\n",
      "<====================== Epoch 18 ======================>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "macro_list = []\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "    # Calculate total loss for this epoch\n",
    "    batch_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda(1) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_init_emo, b_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, init_emo=b_init_emo)\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(weight = torch.FloatTensor([0.6342, 5.9110, 0.8695, 0.5490, 0.4640, 0.8700, 0.7023]).cuda(1))\n",
    "        \n",
    "        # loss_fct = nn.CrossEntropyLoss()        \n",
    "        loss     = loss_fct(outputs, b_labels)\n",
    "        # loss = outputs[0]\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # Gradient clipping is not in AdamW anymore\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate schedule\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear the previous accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update tracking variables\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "    #store the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "        learning_rate.append(param_group['lr'])\n",
    "      \n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "      \n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "    \n",
    "    pred_list = []\n",
    "    labels_list = []\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda(1) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_init_emo, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          logits = model(b_input_ids, attention_mask=b_input_mask, init_emo=b_init_emo)\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.to('cpu').numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        \n",
    "        pred_list = np.append(pred_list, pred_flat)\n",
    "        labels_list = np.append(labels_list, labels_flat)\n",
    "\n",
    "\n",
    "    result = classification_report(pred_list, labels_list, digits=4, output_dict=False)\n",
    "    # print(result)\n",
    "    \n",
    "    print('Test')\n",
    "    pred_list = []\n",
    "    labels_list = []\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda(1) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_init_emo, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          logits = model(b_input_ids, attention_mask=b_input_mask, init_emo=b_init_emo)\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.to('cpu').numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        \n",
    "        pred_list = np.append(pred_list, pred_flat)\n",
    "        labels_list = np.append(labels_list, labels_flat)\n",
    "\n",
    "\n",
    "    result = classification_report(pred_list, labels_list, digits=4, output_dict=False)\n",
    "    dic = classification_report(pred_list, labels_list, digits=4, output_dict=True)\n",
    "    macro_list.append(dic['macro avg']['f1-score'])\n",
    "    print(result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08587570621468925,\n",
       " 0.08587570621468925,\n",
       " 0.11466490657407771,\n",
       " 0.09872862346220392,\n",
       " 0.09204270906398568,\n",
       " 0.09796813462226997,\n",
       " 0.09924577149555765,\n",
       " 0.10137038165450687,\n",
       " 0.11662261664892273,\n",
       " 0.11958529138561512,\n",
       " 0.12301217718072155,\n",
       " 0.12961078918525729,\n",
       " 0.12400293910627837,\n",
       " 0.12277937490785788,\n",
       " 0.14193439778843456,\n",
       " 0.1697131876185775,\n",
       " 0.15785813539081728,\n",
       " 0.1759443854859024,\n",
       " 0.15793485763076415,\n",
       " 0.14285940188140941,\n",
       " 0.1611964277392315,\n",
       " 0.1594931913040873,\n",
       " 0.14799881725920586,\n",
       " 0.14848455616446157,\n",
       " 0.14233899708634262,\n",
       " 0.15587599020332324,\n",
       " 0.15985353723695317,\n",
       " 0.16713670117082788,\n",
       " 0.1605265270814844,\n",
       " 0.16646151298850392,\n",
       " 0.16521365132013993,\n",
       " 0.15744437112259477,\n",
       " 0.1618604790066016,\n",
       " 0.14860607728866262,\n",
       " 0.13913450944089284,\n",
       " 0.15609911631465728,\n",
       " 0.15717668801315152,\n",
       " 0.15978426937381626,\n",
       " 0.158577142899096,\n",
       " 0.16898885308797046,\n",
       " 0.16997583401813035,\n",
       " 0.15875757476289776,\n",
       " 0.17548121473765502,\n",
       " 0.17204696115981713,\n",
       " 0.16248104150730455,\n",
       " 0.1781588766304684,\n",
       " 0.17993444068721093,\n",
       " 0.18164605624614988,\n",
       " 0.17948940279198283,\n",
       " 0.17810737357316525]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
